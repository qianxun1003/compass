# 爬取数据集成方案（小白友好版）

## 🎯 一、核心原则：**完全不影响现有系统**

### ✅ 保证
1. **现有数据完全保留**：爬取的数据不会覆盖或删除现有数据
2. **现有功能正常运行**：所有现有功能继续正常工作
3. **可选择性使用**：可以选择使用爬取数据，也可以继续使用Excel
4. **数据合并策略**：爬取数据作为补充，优先使用现有数据

---

## 📊 二、数据不会"打架"的原因

### 当前系统工作方式：
```
Excel文件 (学部学校一览表.xlsx)
    ↓
export_school_data.py 处理
    ↓
生成 JSON文件 (学校总览.json)
    ↓
前端读取 JSON文件显示
```

### 爬取数据集成方式：
```
方案A：爬取数据 → 生成新Excel → 人工审核 → 合并到现有Excel
方案B：爬取数据 → 生成补充JSON → 前端智能合并显示
```

**关键点**：爬取的数据**不会直接覆盖**现有数据，而是：
1. 生成**独立的文件**（如 `crawled_data.json`）
2. 可以**人工审核**后再决定是否合并
3. 或者前端**智能合并**（爬取数据作为补充，现有数据优先）

---

## 🔄 三、完整流程说明

### 阶段1：爬取数据（我帮您实现）

**您需要做的**：
1. 告诉我目标大学的网站地址
2. 或者给我一个大学列表（我可以帮您找网站）

**我会做的**：
1. 编写爬虫代码
2. 爬取数据并保存为独立文件
3. 生成数据报告（哪些成功、哪些失败）

**输出文件**：
- `crawled_data/crawled_schools.json`（爬取的数据）
- `crawled_data/crawl_report.txt`（爬取报告）

### 阶段2：数据审核（您来做）

**我会生成一个Excel文件**，格式与您现有的 `学部学校一览表.xlsx` 完全一致：

```
crawled_data/
├── crawled_schools_review.xlsx  ← 这个文件您可以打开审核
├── crawled_schools.json         ← 原始数据
└── crawl_report.txt            ← 爬取报告
```

**您可以**：
1. 打开Excel文件查看爬取的数据
2. 修改错误的数据
3. 补充缺失的数据
4. 决定哪些数据要合并到主Excel

### 阶段3：数据合并（可选，我帮您实现）

**方案A：手动合并（推荐，最安全）**
```
1. 打开 crawled_schools_review.xlsx
2. 复制需要的数据行
3. 粘贴到 学部学校一览表.xlsx
4. 运行 python3 export_school_data.py
5. 完成！
```

**方案B：自动合并（我帮您实现脚本）**
```
1. 运行 python3 merge_crawled_data.py
2. 脚本会：
   - 读取爬取的数据
   - 与现有数据对比
   - 只添加新学校/新学部（不覆盖现有数据）
   - 生成合并后的Excel
3. 您审核后运行 export_school_data.py
```

---

## ⏱️ 四、时间成本估算

### 爬取时间估算

**假设条件**：
- 170-180所大学
- 3000多个学部
- 每个大学平均需要爬取2-3个页面
- 每个页面请求延迟1-2秒（避免对服务器造成压力）

**时间计算**：
```
单所大学爬取时间：
- 页面请求：2-3个页面 × 2秒 = 4-6秒
- 数据解析：1-2秒
- 错误重试：预留2秒
- 总计：约7-10秒/大学

全部大学：
- 170所 × 10秒 = 1700秒 ≈ 28分钟（理想情况）
- 考虑网络延迟、错误重试：约1-2小时
- 加上PDF解析（如果有）：可能需要3-4小时
```

**实际建议**：
1. **分批爬取**：每天爬取20-30所大学，一周完成
2. **优先重要大学**：先爬取热门大学（如东大、京大等）
3. **后台运行**：可以在后台运行，不影响您做其他事情

---

## 🛠️ 五、我会帮您实现的工具

### 1. 智能爬虫系统

**特点**：
- ✅ **一键运行**：您只需要运行一个命令
- ✅ **自动重试**：网络错误自动重试
- ✅ **进度显示**：实时显示爬取进度
- ✅ **错误报告**：生成详细的错误报告
- ✅ **数据验证**：自动验证数据格式

**使用方式**：
```bash
# 方式1：爬取所有大学（如果我有大学列表）
python3 crawl_all_universities.py

# 方式2：爬取指定大学
python3 crawl_university.py --url "https://www.u-tokyo.ac.jp/admission/"

# 方式3：从Excel读取大学列表并爬取
python3 crawl_from_excel.py --input "学部学校一览表.xlsx"
```

### 2. 数据审核工具

**生成Excel文件**，方便您审核：
```bash
python3 generate_review_excel.py
# 生成：crawled_data/crawled_schools_review.xlsx
```

### 3. 数据合并工具

**安全合并数据**：
```bash
python3 merge_crawled_data.py --reviewed "crawled_schools_review.xlsx"
# 只添加新数据，不覆盖现有数据
```

---

## 📋 六、详细流程示例

### 示例：爬取东京大学的数据

**步骤1：我帮您编写爬虫**
```python
# 我会创建 scripts/crawlers/tokyo_univ.py
# 您不需要懂代码，只需要告诉我网站地址
```

**步骤2：运行爬虫**
```bash
# 您只需要运行：
python3 scripts/crawl_university.py --name "東京大学" --url "https://www.u-tokyo.ac.jp/admission/"
```

**步骤3：查看结果**
```
输出：
✅ 成功爬取：東京大学 - 6个学部
📄 数据已保存到：crawled_data/tokyo_univ_20250217.json
📊 审核文件：crawled_data/tokyo_univ_review.xlsx
```

**步骤4：审核数据**
```
1. 打开 tokyo_univ_review.xlsx
2. 检查数据是否正确
3. 修改错误的数据
4. 保存文件
```

**步骤5：合并数据（可选）**
```bash
# 如果数据正确，运行合并：
python3 merge_crawled_data.py --file "tokyo_univ_review.xlsx"
# 输出：✅ 已添加6个新学部到主Excel
```

**步骤6：更新系统**
```bash
# 运行现有的导出脚本：
python3 export_school_data.py
# 完成！系统已更新
```

---

## 🔒 七、安全保障机制

### 1. 数据备份
每次合并前自动备份：
```
backups/
├── 学校总览_20250217_143022.json  ← 自动备份
└── 学校总览_20250217_143022.csv
```

### 2. 冲突检测
合并时会检测：
- ✅ 是否已有相同大学+学部
- ✅ 如果存在，询问是否覆盖（默认不覆盖）
- ✅ 只添加新数据

### 3. 数据验证
自动验证：
- ✅ 必需字段是否完整
- ✅ 日期格式是否正确
- ✅ 数据格式是否符合要求

---

## 💡 八、推荐实施方案

### 方案1：保守方案（最安全，推荐）

**流程**：
1. 我帮您爬取数据 → 生成独立文件
2. 您审核Excel文件
3. 您手动复制到主Excel（最安全）
4. 运行现有导出脚本

**优点**：
- ✅ 完全控制
- ✅ 不会出错
- ✅ 可以逐步合并

### 方案2：半自动方案

**流程**：
1. 我帮您爬取数据 → 生成独立文件
2. 您审核Excel文件
3. 运行合并脚本（自动，但会询问确认）
4. 运行现有导出脚本

**优点**：
- ✅ 节省时间
- ✅ 有安全保障
- ✅ 可以批量处理

---

## 📝 九、您需要提供的信息

### 最小信息需求：
1. **大学列表**（如果有Excel最好）
   - 或者告诉我：我需要爬取哪些大学

2. **网站地址**（可选）
   - 如果不知道，我可以帮您找
   - 或者告诉我：您希望我从哪里找这些信息

### 我会帮您做的：
1. ✅ 编写所有爬虫代码
2. ✅ 处理PDF文件
3. ✅ 数据清洗和格式化
4. ✅ 生成审核Excel
5. ✅ 实现合并工具
6. ✅ 编写使用说明

---

## 🎓 十、使用示例（完全不需要编程知识）

### 场景：您想爬取10所热门大学

**第1步：告诉我大学名称**
```
您：我想爬取东京大学、京都大学、大阪大学...
我：好的，我来帮您编写爬虫
```

**第2步：我给您一个简单的命令**
```bash
# 我给您这个命令，您只需要复制粘贴运行：
python3 crawl_top_universities.py
```

**第3步：等待完成**
```
运行后您会看到：
[1/10] 正在爬取：東京大学... ✅ 完成
[2/10] 正在爬取：京都大学... ✅ 完成
...
[10/10] 全部完成！

📊 统计：
- 成功：10所大学
- 失败：0所
- 总学部数：45个
- 数据文件：crawled_data/top_universities_20250217.json
- 审核文件：crawled_data/top_universities_review.xlsx
```

**第4步：打开Excel审核**
```
打开：crawled_data/top_universities_review.xlsx
检查数据，修改错误
保存文件
```

**第5步：合并数据（可选）**
```bash
# 如果数据正确，运行：
python3 merge_crawled_data.py --file "top_universities_review.xlsx"
```

**第6步：更新系统**
```bash
python3 export_school_data.py
```

**完成！** 🎉

---

## ❓ 十一、常见问题

### Q1: 爬取的数据会覆盖我现有的数据吗？
**A**: 不会！爬取的数据保存在独立文件中，只有您明确同意合并时才会合并。

### Q2: 如果爬取失败怎么办？
**A**: 会生成详细的错误报告，您可以：
- 查看哪些大学爬取失败
- 手动补充失败的数据
- 或者告诉我，我帮您修复爬虫

### Q3: 爬取需要多长时间？
**A**: 
- 单所大学：约10秒
- 170所大学：约1-2小时（可以分批进行）
- 建议：每天爬取20-30所，一周完成

### Q4: 我需要懂编程吗？
**A**: 完全不需要！我会给您简单的命令，您只需要复制粘贴运行。

### Q5: 如果网站改版了怎么办？
**A**: 告诉我，我会帮您更新爬虫代码。

### Q6: 爬取会影响网站吗？
**A**: 不会！我会设置合理的延迟，不会对服务器造成压力。

---

## 🚀 十二、下一步行动

### 如果您同意，我可以：

1. **立即开始**：
   - 您给我一个大学列表（或告诉我爬取哪些大学）
   - 我帮您编写爬虫代码
   - 实现所有工具

2. **先做试点**：
   - 选择2-3所大学作为试点
   - 测试整个流程
   - 确认没问题后扩展到所有大学

3. **您先考虑**：
   - 您可以先看看这个方案
   - 有任何问题随时问我
   - 准备好了再开始

**您希望我怎么做？** 🤔
