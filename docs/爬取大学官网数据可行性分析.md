# çˆ¬å–å¤§å­¦å®˜ç½‘æ•°æ®å¯è¡Œæ€§åˆ†æ

## ğŸ“Š ä¸€ã€å¯è¡Œæ€§è¯„ä¼°

### âœ… æŠ€æœ¯å¯è¡Œæ€§ï¼š**å¯è¡Œï¼Œä½†æœ‰ä¸€å®šæŒ‘æˆ˜**

çˆ¬å–å¤§å­¦å®˜ç½‘æ•°æ®åœ¨æŠ€æœ¯ä¸Šæ˜¯å¯è¡Œçš„ï¼Œä½†éœ€è¦å¤„ç†ä»¥ä¸‹æŒ‘æˆ˜ï¼š

### âš ï¸ ä¸»è¦æŒ‘æˆ˜

1. **ç½‘ç«™ç»“æ„å¤šæ ·åŒ–**
   - æ¯ä¸ªå¤§å­¦çš„ç½‘ç«™ç»“æ„ä¸åŒ
   - ä¿¡æ¯å¯èƒ½åˆ†æ•£åœ¨å¤šä¸ªé¡µé¢
   - éœ€è¦ä¸ºæ¯ä¸ªå¤§å­¦ç¼–å†™ç‰¹å®šçš„çˆ¬è™«é€»è¾‘

2. **æ•°æ®æ ¼å¼å¤šæ ·**
   - HTMLé¡µé¢
   - PDFæ–‡ä»¶ï¼ˆå¾ˆå¤šå¤§å­¦ç”¨PDFå‘å¸ƒæ‹›ç”Ÿç®€ç« ï¼‰
   - è¡¨æ ¼æ•°æ®
   - åŠ¨æ€åŠ è½½çš„å†…å®¹ï¼ˆJavaScriptæ¸²æŸ“ï¼‰

3. **ä¿¡æ¯å®Œæ•´æ€§**
   - å®˜ç½‘å¯èƒ½ä¸åŒ…å«æ‰€æœ‰éœ€è¦çš„ä¿¡æ¯
   - æŸäº›ä¿¡æ¯å¯èƒ½éœ€è¦ç™»å½•æˆ–ç‰¹æ®Šæƒé™
   - ä¿¡æ¯å¯èƒ½åœ¨ä¸åŒå¹´ä»½çš„é¡µé¢ä¸­

4. **æ³•å¾‹å’Œä¼¦ç†**
   - éœ€è¦éµå®ˆ `robots.txt`
   - éœ€è¦éµå®ˆç½‘ç«™ä½¿ç”¨æ¡æ¬¾
   - éœ€è¦åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›

5. **ç»´æŠ¤æˆæœ¬**
   - ç½‘ç«™ç»“æ„å˜åŒ–éœ€è¦æ›´æ–°çˆ¬è™«
   - éœ€è¦å®šæœŸè¿è¡Œçˆ¬è™«æ›´æ–°æ•°æ®
   - éœ€è¦å¤„ç†ç½‘ç«™æ”¹ç‰ˆã€é¡µé¢å¤±æ•ˆç­‰æƒ…å†µ

---

## ğŸ“‹ äºŒã€éœ€è¦çˆ¬å–çš„æ•°æ®å­—æ®µ

æ ¹æ®å½“å‰ç³»ç»Ÿéœ€æ±‚ï¼Œéœ€è¦çˆ¬å–ä»¥ä¸‹æ•°æ®ï¼š

### åŸºç¡€ä¿¡æ¯
- âœ… å¤§å­¦åç§°ï¼ˆ`name`ï¼‰
- âœ… å­¦éƒ¨åç§°ï¼ˆ`department`ï¼‰
- âœ… å­¦ç§‘åç§°ï¼ˆ`major`ï¼‰
- âœ… åœ°ç†ä½ç½®ï¼ˆ`region`ï¼‰
- âœ… æ–‡ç†åˆ†ç±»ï¼ˆ`bunri`ï¼‰

### å…¥è¯•ä¿¡æ¯
- âœ… å…¥è¯•æ–¹å¼ï¼ˆ`selectionMethod`ï¼‰
- âœ… å‡ºæ„¿æœŸæ•°ï¼ˆ`period`ï¼‰
- âœ… æ˜¯å¦å¯ä½µé¡˜ï¼ˆ`combined`ï¼‰

### EJUç›¸å…³
- âœ… èƒ½ä½¿ç”¨çš„EJUæ—¶æœŸï¼ˆ`ejuPeriod`ï¼‰
- âœ… éœ€è¦çš„EJUç§‘ç›®ï¼ˆ`ejuSubjects`ï¼‰

### è¯­è¨€è¦æ±‚
- âœ… æ˜¯å¦éœ€è¦è‹±è¯­ï¼ˆ`english`ï¼‰
- âœ… æ˜¯å¦éœ€è¦JLPTï¼ˆ`jlpt`ï¼‰

### æ—¶é—´ä¿¡æ¯ï¼ˆæ¯å¹´éƒ½ä¼šå˜åŒ–ï¼‰
- âœ… ç½‘ä¸Šå‡ºæ„¿å¼€å§‹æ—¶é—´ï¼ˆ`mailStart`ï¼‰
- âœ… ç½‘ä¸Šå‡ºæ„¿æˆªæ­¢æ—¶é—´ï¼ˆ`mailEnd`ï¼‰
- âœ… é‚®å¯„å¼€å§‹æ—¶é—´ï¼ˆ`mailStartDate`ï¼‰
- âœ… é‚®å¯„æˆªæ­¢æ—¶é—´ï¼ˆ`mailEndDate`ï¼‰
- âœ… é‚®å¯„æˆªæ­¢è¯´æ˜ï¼ˆ`mailEndNote`ï¼šå¿…ç€/æ¶ˆå°ï¼‰
- âœ… æ ¡å†…è€ƒæ—¶é—´1ï¼ˆ`examDate`ï¼‰
- âœ… æ ¡å†…è€ƒæ—¶é—´2ï¼ˆ`examDate2`ï¼‰
- âœ… å‘æ¦œæ—¶é—´ï¼ˆ`announcementDate`ï¼‰

### å…¶ä»–
- âœ… æ ¡å†…è€ƒå½¢å¼ï¼ˆ`examFormat`ï¼‰
- âœ… ç‰¹æ®Šæˆç»©è¦æ±‚ï¼ˆ`specialRequirements`ï¼‰

---

## ğŸ¯ ä¸‰ã€å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šå…¨è‡ªåŠ¨çˆ¬å–ï¼ˆæ¨èç”¨äºè¯•ç‚¹ï¼‰

**ä¼˜ç‚¹**ï¼š
- å®Œå…¨è‡ªåŠ¨åŒ–
- å¯ä»¥å®šæœŸæ›´æ–°

**ç¼ºç‚¹**ï¼š
- å¼€å‘æˆæœ¬é«˜
- ç»´æŠ¤æˆæœ¬é«˜
- éœ€è¦å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ

**å®ç°æ­¥éª¤**ï¼š
1. å»ºç«‹å¤§å­¦URLåˆ—è¡¨å’Œé¡µé¢ç»“æ„é…ç½®
2. ä¸ºæ¯ä¸ªå¤§å­¦ç¼–å†™çˆ¬è™«é€»è¾‘ï¼ˆæˆ–é€šç”¨è§£æå™¨ï¼‰
3. å¤„ç†PDFæ–‡ä»¶ï¼ˆä½¿ç”¨PDFè§£æåº“ï¼‰
4. æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–
5. å¯¼å‡ºä¸ºJSONæ ¼å¼

### æ–¹æ¡ˆBï¼šåŠè‡ªåŠ¨çˆ¬å– + äººå·¥å®¡æ ¸ï¼ˆæ¨èï¼‰

**ä¼˜ç‚¹**ï¼š
- å¹³è¡¡è‡ªåŠ¨åŒ–å’Œå‡†ç¡®æ€§
- å¯ä»¥å¤„ç†å¤æ‚æƒ…å†µ
- é™ä½ç»´æŠ¤æˆæœ¬

**ç¼ºç‚¹**ï¼š
- éœ€è¦äººå·¥å®¡æ ¸
- ä¸èƒ½å®Œå…¨è‡ªåŠ¨åŒ–

**å®ç°æ­¥éª¤**ï¼š
1. çˆ¬è™«è‡ªåŠ¨æŠ“å–åŸºç¡€ä¿¡æ¯
2. ç”ŸæˆExcelæ–‡ä»¶ä¾›äººå·¥å®¡æ ¸å’Œè¡¥å……
3. äººå·¥å®¡æ ¸åå¯¼å…¥ç³»ç»Ÿ

### æ–¹æ¡ˆCï¼šAPIé›†æˆï¼ˆå¦‚æœæœ‰ï¼‰

**ä¼˜ç‚¹**ï¼š
- æœ€å¯é 
- æ•°æ®æ ¼å¼ç»Ÿä¸€
- ç»´æŠ¤æˆæœ¬ä½

**ç¼ºç‚¹**ï¼š
- å¤§å¤šæ•°å¤§å­¦ä¸æä¾›å…¬å¼€API
- éœ€è¦ä¸å¤§å­¦åå•†

---

## ğŸ› ï¸ å››ã€æŠ€æœ¯å®ç°å»ºè®®

### æ¨èæŠ€æœ¯æ ˆ

```python
# ç½‘é¡µçˆ¬å–
- requestsï¼šHTTPè¯·æ±‚
- BeautifulSoup4ï¼šHTMLè§£æ
- Seleniumï¼šå¤„ç†JavaScriptæ¸²æŸ“çš„é¡µé¢
- Scrapyï¼šå¤§å‹çˆ¬è™«æ¡†æ¶ï¼ˆå¯é€‰ï¼‰

# PDFå¤„ç†
- PyPDF2 / pdfplumberï¼šPDFæ–‡æœ¬æå–
- pdfminerï¼šPDFè§£æ

# æ•°æ®å¤„ç†
- pandasï¼šæ•°æ®å¤„ç†
- dateparserï¼šæ—¥æœŸè§£æï¼ˆæ”¯æŒæ—¥æ–‡æ—¥æœŸæ ¼å¼ï¼‰

# æ•°æ®å­˜å‚¨
- ä¸´æ—¶å­˜å‚¨ï¼šJSON / CSV
- æœ€ç»ˆè¾“å‡ºï¼šä¸ç°æœ‰ç³»ç»Ÿå…¼å®¹çš„æ ¼å¼
```

### çˆ¬è™«æ¶æ„å»ºè®®

```
scripts/
â”œâ”€â”€ crawlers/
â”‚   â”œâ”€â”€ base_crawler.py          # åŸºç¡€çˆ¬è™«ç±»
â”‚   â”œâ”€â”€ tokyo_univ_crawler.py   # ä¸œäº¬å¤§å­¦çˆ¬è™«
â”‚   â”œâ”€â”€ kyoto_univ_crawler.py   # äº¬éƒ½å¤§å­¦çˆ¬è™«
â”‚   â””â”€â”€ ...                      # å…¶ä»–å¤§å­¦çˆ¬è™«
â”œâ”€â”€ parsers/
â”‚   â”œâ”€â”€ pdf_parser.py            # PDFè§£æå™¨
â”‚   â”œâ”€â”€ html_parser.py           # HTMLè§£æå™¨
â”‚   â””â”€â”€ date_parser.py           # æ—¥æœŸè§£æå™¨
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_cleaner.py          # æ•°æ®æ¸…æ´—
â”‚   â””â”€â”€ export.py                # æ•°æ®å¯¼å‡º
â””â”€â”€ crawl_all.py                 # ä¸»å…¥å£è„šæœ¬
```

---

## ğŸ“ äº”ã€å®ç°ç¤ºä¾‹ä»£ç 

### åŸºç¡€çˆ¬è™«æ¡†æ¶ç¤ºä¾‹

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤§å­¦å®˜ç½‘æ•°æ®çˆ¬å–æ¡†æ¶
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
from pathlib import Path
import time
import re

class BaseUniversityCrawler:
    """åŸºç¡€å¤§å­¦çˆ¬è™«ç±»"""
    
    def __init__(self, university_name, base_url):
        self.university_name = university_name
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.data = []
    
    def crawl(self):
        """ä¸»çˆ¬å–æ–¹æ³•ï¼Œå­ç±»éœ€è¦å®ç°"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° crawl æ–¹æ³•")
    
    def parse_admission_info(self, html_content):
        """è§£ææ‹›ç”Ÿä¿¡æ¯ï¼Œå­ç±»éœ€è¦å®ç°"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° parse_admission_info æ–¹æ³•")
    
    def parse_date(self, date_str):
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²ä¸ºç»Ÿä¸€æ ¼å¼"""
        # ä½¿ç”¨ dateparser æˆ–è‡ªå®šä¹‰è§£æé€»è¾‘
        # è¿”å›æ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS
        pass
    
    def save_to_json(self, output_path):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        output = {
            "university": self.university_name,
            "crawled_at": datetime.now().isoformat(),
            "data": self.data
        }
        Path(output_path).write_text(
            json.dumps(output, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )
    
    def delay(self, seconds=1):
        """å»¶è¿Ÿè¯·æ±‚ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›"""
        time.sleep(seconds)


class TokyoUniversityCrawler(BaseUniversityCrawler):
    """ä¸œäº¬å¤§å­¦çˆ¬è™«ç¤ºä¾‹"""
    
    def __init__(self):
        super().__init__(
            university_name="æ±äº¬å¤§å­¦",
            base_url="https://www.u-tokyo.ac.jp"
        )
        self.admission_url = "https://www.u-tokyo.ac.jp/admission/undergraduate/foreign.html"
    
    def crawl(self):
        """çˆ¬å–ä¸œäº¬å¤§å­¦æ‹›ç”Ÿä¿¡æ¯"""
        try:
            response = self.session.get(self.admission_url, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                self.parse_admission_info(soup)
            else:
                print(f"æ— æ³•è®¿é—®é¡µé¢: {self.admission_url}")
        
        except Exception as e:
            print(f"çˆ¬å–å¤±è´¥: {e}")
        
        self.delay(2)  # å»¶è¿Ÿ2ç§’
    
    def parse_admission_info(self, soup):
        """è§£ææ‹›ç”Ÿä¿¡æ¯é¡µé¢"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„ç¼–å†™è§£æé€»è¾‘
        # ç¤ºä¾‹ï¼š
        departments = soup.find_all('div', class_='department')  # å‡è®¾çš„CSSé€‰æ‹©å™¨
        
        for dept in departments:
            info = {
                "name": self.university_name,
                "department": dept.find('h3').text.strip(),
                "region": "æ±äº¬éƒ½",
                # ... å…¶ä»–å­—æ®µ
            }
            self.data.append(info)


def main():
    """ä¸»å‡½æ•°"""
    crawlers = [
        TokyoUniversityCrawler(),
        # æ·»åŠ å…¶ä»–å¤§å­¦çˆ¬è™«
    ]
    
    all_data = []
    for crawler in crawlers:
        print(f"æ­£åœ¨çˆ¬å–: {crawler.university_name}")
        crawler.crawl()
        all_data.extend(crawler.data)
    
    # å¯¼å‡ºä¸ºä¸ç°æœ‰ç³»ç»Ÿå…¼å®¹çš„æ ¼å¼
    output = {"data": all_data}
    Path("crawled_school_data.json").write_text(
        json.dumps(output, ensure_ascii=False, indent=2),
        encoding='utf-8'
    )
    print(f"å·²çˆ¬å– {len(all_data)} æ¡æ•°æ®")


if __name__ == "__main__":
    main()
```

### PDFè§£æç¤ºä¾‹

```python
import pdfplumber
import re

def parse_pdf_admission_info(pdf_path):
    """è§£æPDFæ ¼å¼çš„æ‹›ç”Ÿç®€ç« """
    data = []
    
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®ä¿¡æ¯
            # ä¾‹å¦‚ï¼šæå–æ—¥æœŸ
            date_pattern = r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥'
            dates = re.findall(date_pattern, text)
            
            # æå–EJUç§‘ç›®è¦æ±‚
            eju_pattern = r'EJU.*?ç§‘ç›®[ï¼š:]\s*([^ã€‚\n]+)'
            eju_subjects = re.findall(eju_pattern, text)
            
            # ... å…¶ä»–è§£æé€»è¾‘
    
    return data
```

---

## âš ï¸ å…­ã€æ³¨æ„äº‹é¡¹å’Œæœ€ä½³å®è·µ

### 1. éµå®ˆrobots.txt
```python
from urllib.robotparser import RobotFileParser

def can_fetch(url, user_agent='*'):
    rp = RobotFileParser()
    rp.set_url(f"{url}/robots.txt")
    rp.read()
    return rp.can_fetch(user_agent, url)
```

### 2. è®¾ç½®åˆç†çš„è¯·æ±‚é—´éš”
```python
import time
import random

def delay(min_seconds=1, max_seconds=3):
    """éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
    time.sleep(random.uniform(min_seconds, max_seconds))
```

### 3. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def fetch_with_retry(url):
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    return response
```

### 4. æ•°æ®éªŒè¯
```python
def validate_school_data(data):
    """éªŒè¯çˆ¬å–çš„æ•°æ®æ˜¯å¦ç¬¦åˆè¦æ±‚"""
    required_fields = ['name', 'department', 'region', 'bunri']
    
    for field in required_fields:
        if field not in data or not data[field]:
            return False, f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
    
    # éªŒè¯æ—¥æœŸæ ¼å¼
    date_fields = ['mailStart', 'mailEnd', 'examDate']
    for field in date_fields:
        if field in data:
            try:
                datetime.strptime(data[field], "%Y-%m-%d %H:%M:%S")
            except ValueError:
                return False, f"æ—¥æœŸæ ¼å¼é”™è¯¯: {field}"
    
    return True, "éªŒè¯é€šè¿‡"
```

---

## ğŸ¯ ä¸ƒã€æ¨èå®æ–½æ–¹æ¡ˆ

### é˜¶æ®µ1ï¼šè¯•ç‚¹çˆ¬å–ï¼ˆ1-2å‘¨ï¼‰
1. é€‰æ‹©3-5æ‰€å¤§å­¦ä½œä¸ºè¯•ç‚¹
2. åˆ†æè¿™äº›å¤§å­¦çš„ç½‘ç«™ç»“æ„
3. ç¼–å†™çˆ¬è™«è„šæœ¬
4. æµ‹è¯•æ•°æ®å‡†ç¡®æ€§

### é˜¶æ®µ2ï¼šæ‰©å±•çˆ¬å–ï¼ˆ2-4å‘¨ï¼‰
1. æ ¹æ®è¯•ç‚¹ç»éªŒï¼Œæ‰©å±•æ›´å¤šå¤§å­¦
2. å»ºç«‹çˆ¬è™«é…ç½®ç³»ç»Ÿï¼ˆæ¯ä¸ªå¤§å­¦çš„URLå’Œè§£æè§„åˆ™ï¼‰
3. å®ç°PDFè§£æåŠŸèƒ½
4. å®ç°æ•°æ®æ¸…æ´—å’ŒéªŒè¯

### é˜¶æ®µ3ï¼šé›†æˆåˆ°ç°æœ‰ç³»ç»Ÿï¼ˆ1å‘¨ï¼‰
1. ä¿®æ”¹ `export_school_data.py` æ”¯æŒä»çˆ¬å–æ•°æ®å¯¼å…¥
2. æˆ–è€…åˆ›å»ºæ–°çš„è„šæœ¬ `import_crawled_data.py`
3. å®ç°æ•°æ®åˆå¹¶é€»è¾‘ï¼ˆçˆ¬å–æ•°æ® + äººå·¥è¡¥å……æ•°æ®ï¼‰

### é˜¶æ®µ4ï¼šè‡ªåŠ¨åŒ–ï¼ˆå¯é€‰ï¼‰
1. è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆå¦‚æ¯å‘¨è¿è¡Œä¸€æ¬¡ï¼‰
2. å®ç°æ•°æ®å˜æ›´é€šçŸ¥
3. å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§

---

## ğŸ“Š å…«ã€æ•°æ®è´¨é‡å¯¹æ¯”

| æ•°æ®æ¥æº | å‡†ç¡®æ€§ | å®Œæ•´æ€§ | æ›´æ–°é¢‘ç‡ | ç»´æŠ¤æˆæœ¬ |
|---------|--------|--------|---------|---------|
| Excelæ‰‹åŠ¨è¾“å…¥ | â­â­â­â­â­ | â­â­â­â­â­ | æ‰‹åŠ¨ | ä½ |
| å®˜ç½‘çˆ¬å– | â­â­â­ | â­â­â­ | è‡ªåŠ¨ | é«˜ |
| åŠè‡ªåŠ¨çˆ¬å–+äººå·¥å®¡æ ¸ | â­â­â­â­ | â­â­â­â­ | åŠè‡ªåŠ¨ | ä¸­ |

---

## ğŸ’¡ ä¹ã€å»ºè®®

### æ¨èæ–¹æ¡ˆï¼š**åŠè‡ªåŠ¨çˆ¬å– + äººå·¥å®¡æ ¸**

**ç†ç”±**ï¼š
1. âœ… å¹³è¡¡äº†è‡ªåŠ¨åŒ–å’Œå‡†ç¡®æ€§
2. âœ… å¯ä»¥å¤„ç†å¤æ‚æƒ…å†µï¼ˆPDFã€ç‰¹æ®Šæ ¼å¼ç­‰ï¼‰
3. âœ… é™ä½ç»´æŠ¤æˆæœ¬
4. âœ… å¯ä»¥é€æ­¥æ‰©å±•

**å®æ–½æ­¥éª¤**ï¼š
1. å…ˆçˆ¬å–åŸºç¡€ä¿¡æ¯ï¼ˆå¤§å­¦åã€å­¦éƒ¨ã€ä½ç½®ç­‰ç›¸å¯¹ç¨³å®šçš„ä¿¡æ¯ï¼‰
2. æ—¶é—´ä¿¡æ¯ï¼ˆæ¯å¹´å˜åŒ–ï¼‰å¯ä»¥çˆ¬å–åäººå·¥å®¡æ ¸
3. ç”ŸæˆExcelæ–‡ä»¶ï¼Œä¸ç°æœ‰Excelæ ¼å¼å…¼å®¹
4. äººå·¥å®¡æ ¸åï¼Œä½¿ç”¨ç°æœ‰çš„ `export_school_data.py` å¯¼å…¥

### ä¸æ¨èï¼šå®Œå…¨è‡ªåŠ¨åŒ–çˆ¬å–

**ç†ç”±**ï¼š
1. âŒ å¼€å‘æˆæœ¬é«˜ï¼ˆéœ€è¦ä¸ºæ¯ä¸ªå¤§å­¦ç¼–å†™ç‰¹å®šé€»è¾‘ï¼‰
2. âŒ ç»´æŠ¤æˆæœ¬é«˜ï¼ˆç½‘ç«™æ”¹ç‰ˆéœ€è¦æ›´æ–°ï¼‰
3. âŒ æ•°æ®å‡†ç¡®æ€§éš¾ä»¥ä¿è¯ï¼ˆç‰¹åˆ«æ˜¯æ—¥æœŸã€ç‰¹æ®Šè¦æ±‚ç­‰ï¼‰
4. âŒ å¯èƒ½è¿åç½‘ç«™ä½¿ç”¨æ¡æ¬¾

---

## ğŸ”— åã€ç›¸å…³èµ„æº

- [Scrapyæ–‡æ¡£](https://docs.scrapy.org/)
- [BeautifulSoupæ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [pdfplumberæ–‡æ¡£](https://github.com/jsvine/pdfplumber)
- [æ—¥æœ¬å¤§å­¦æ‹›ç”Ÿä¿¡æ¯ç½‘ç«™ç¤ºä¾‹](https://www.u-tokyo.ac.jp/admission/)

---

## ğŸ“ åä¸€ã€ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç¡®å®šç›®æ ‡å¤§å­¦åˆ—è¡¨**ï¼ˆä¼˜å…ˆçˆ¬å–å“ªäº›å¤§å­¦ï¼‰
2. **åˆ†æç›®æ ‡å¤§å­¦ç½‘ç«™ç»“æ„**ï¼ˆæ‰¾åˆ°æ‹›ç”Ÿä¿¡æ¯é¡µé¢ï¼‰
3. **ç¼–å†™è¯•ç‚¹çˆ¬è™«**ï¼ˆé€‰æ‹©1-2æ‰€å¤§å­¦ä½œä¸ºè¯•ç‚¹ï¼‰
4. **æµ‹è¯•å’Œä¼˜åŒ–**ï¼ˆéªŒè¯æ•°æ®å‡†ç¡®æ€§ï¼‰
5. **æ‰©å±•åˆ°æ›´å¤šå¤§å­¦**ï¼ˆé€æ­¥å¢åŠ ï¼‰

å¦‚æœéœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®æ‚¨ï¼š
- ç¼–å†™å…·ä½“çš„çˆ¬è™«ä»£ç 
- åˆ†æç‰¹å®šå¤§å­¦çš„ç½‘ç«™ç»“æ„
- è®¾è®¡æ•°æ®å¯¼å…¥æµç¨‹
- å®ç°PDFè§£æåŠŸèƒ½
