# 爬取数据流程简单说明（小白版）

## 🎯 核心保证

**✅ 完全不影响现有系统**
- 爬取的数据保存在**独立文件夹**中
- **不会自动覆盖**您现有的数据
- 只有您**明确同意**时才会合并
- 合并前会**自动备份**您的Excel文件

---

## 📋 完整流程（用最简单的话说）

### 第1步：我帮您爬取数据

**您需要做的**：
- 告诉我：您想爬取哪些大学？
- 或者：给我一个大学列表（Excel或文本都可以）

**我会做的**：
- 编写爬虫代码（您不需要懂代码）
- 爬取数据
- 保存到独立文件夹：`crawled_data/`

**输出**：
```
crawled_data/
├── crawled_schools.json          ← 原始数据（JSON格式）
├── crawled_schools_review.xlsx   ← 审核文件（Excel格式，您可以打开）
└── crawl_report.txt              ← 爬取报告（哪些成功、哪些失败）
```

### 第2步：您审核数据

**您需要做的**：
1. 打开 `crawled_data/crawled_schools_review.xlsx`
2. 检查数据是否正确
3. 修改错误的数据
4. 补充缺失的数据
5. 保存文件

**就像您平时编辑Excel一样！**

### 第3步：合并数据（可选）

**方式A：手动合并（最安全，推荐）**
```
1. 打开 crawled_data/crawled_schools_review.xlsx
2. 复制需要的数据行（Ctrl+C）
3. 打开 学部学校一览表.xlsx
4. 粘贴到末尾（Ctrl+V）
5. 保存
6. 运行：python3 export_school_data.py
7. 完成！
```

**方式B：自动合并（我帮您实现）**
```
1. 运行：python3 scripts/merge_crawled_data.py
2. 脚本会：
   - 自动备份您的Excel
   - 只添加新数据（不覆盖现有数据）
   - 询问您确认
3. 确认后运行：python3 export_school_data.py
4. 完成！
```

---

## ⏱️ 时间估算

### 爬取时间

**单所大学**：约10秒
- 访问网站：2-3秒
- 解析数据：1-2秒
- 保存数据：1秒
- 延迟（避免对服务器造成压力）：2-3秒

**全部170所大学**：
- 理想情况：170 × 10秒 = 1700秒 ≈ **28分钟**
- 实际情况（考虑网络延迟、错误重试）：**1-2小时**

**建议**：
- 可以分批爬取：每天爬20-30所，一周完成
- 可以后台运行：不影响您做其他事情
- 可以先爬热门大学：东大、京大、早稻田等

---

## 🔒 数据不会"打架"的原因

### 现在的系统流程：
```
您的Excel文件
    ↓
export_school_data.py 处理
    ↓
生成 JSON文件
    ↓
网站显示数据
```

### 爬取数据流程：
```
爬虫爬取数据
    ↓
保存到独立文件夹（不影响您的Excel）
    ↓
您审核Excel文件
    ↓
您决定是否合并
    ↓
如果合并，才更新您的Excel
```

**关键点**：
1. 爬取的数据**不会直接**写入您的Excel
2. 保存在**独立文件夹**中
3. 您可以**慢慢审核**
4. 只有您**确认**后才会合并
5. 合并前会**自动备份**

---

## 💡 实际使用示例

### 场景：您想爬取10所热门大学

**第1步：告诉我大学名称**
```
您：我想爬取东京大学、京都大学、大阪大学、名古屋大学、东北大学、
    九州大学、北海道大学、一桥大学、早稻田大学、庆应义塾大学

我：好的，我来帮您编写爬虫代码
```

**第2步：我给您一个简单的命令**
```bash
# 我给您这个命令，您只需要复制粘贴运行：
python3 scripts/crawl_universities.py --list "tokyo,kyoto,osaka,nagoya,tohoku,kyushu,hokkaido,hitotsubashi,waseda,keio"
```

**第3步：等待完成（约2-3分钟）**
```
运行后您会看到：
[1/10] 正在爬取：東京大学... ✅ 完成
[2/10] 正在爬取：京都大学... ✅ 完成
...
[10/10] 全部完成！

📊 统计：
- 成功：10所大学
- 失败：0所
- 总学部数：45个
- 数据文件：crawled_data/crawled_schools_20250217.json
- 审核文件：crawled_data/crawled_schools_review.xlsx
```

**第4步：打开Excel审核**
```
1. 打开：crawled_data/crawled_schools_review.xlsx
2. 检查数据，修改错误
3. 保存文件
```

**第5步：合并数据（可选）**
```bash
# 如果数据正确，运行：
python3 scripts/merge_crawled_data.py

# 会显示：
📊 统计:
   - 新数据: 45 条
   - 重复数据: 0 条（已跳过）

⚠️  准备添加 45 条新数据到主Excel
确认合并？(y/n): y

✅ 合并完成！
   - 原数据: 3000 条
   - 新增: 45 条
   - 总计: 3045 条
```

**第6步：更新系统**
```bash
python3 export_school_data.py
# 输出：已导出 3045 条到 学校总览.json
```

**完成！** 🎉

---

## ❓ 常见问题

### Q1: 爬取的数据会覆盖我现有的数据吗？
**A**: **绝对不会！** 爬取的数据保存在独立文件夹中，只有您明确同意合并时才会合并。

### Q2: 如果爬取失败怎么办？
**A**: 会生成详细的错误报告，您可以：
- 查看哪些大学爬取失败
- 手动补充失败的数据
- 或者告诉我，我帮您修复爬虫

### Q3: 爬取需要多长时间？
**A**: 
- 单所大学：约10秒
- 170所大学：约1-2小时（可以分批进行）
- 建议：每天爬20-30所，一周完成

### Q4: 我需要懂编程吗？
**A**: **完全不需要！** 我会给您简单的命令，您只需要复制粘贴运行。

### Q5: 如果网站改版了怎么办？
**A**: 告诉我，我会帮您更新爬虫代码。

### Q6: 爬取会影响网站吗？
**A**: 不会！我会设置合理的延迟（每次请求间隔2-3秒），不会对服务器造成压力。

### Q7: 合并时如果数据重复怎么办？
**A**: 脚本会自动检测重复（基于大学+学部），只添加新数据，不会覆盖现有数据。

### Q8: 如果合并出错了怎么办？
**A**: 合并前会自动备份您的Excel文件，如果出错可以恢复备份。

---

## 🚀 下一步

### 如果您同意，我可以：

1. **立即开始**：
   - 您给我一个大学列表（或告诉我爬取哪些大学）
   - 我帮您编写爬虫代码
   - 实现所有工具

2. **先做试点**：
   - 选择2-3所大学作为试点
   - 测试整个流程
   - 确认没问题后扩展到所有大学

3. **您先考虑**：
   - 您可以先看看这个方案
   - 有任何问题随时问我
   - 准备好了再开始

**您希望我怎么做？** 🤔
