#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€çˆ¬å–æ¡†æ¶ï¼šä¸€æ¬¡æ€§çˆ¬å–æ‰€æœ‰éœ€è¦çš„æ•°æ®
åŒ…æ‹¬ï¼šåŸºç¡€ä¿¡æ¯ã€æœŸæ•°ã€é€‰è€ƒæ–¹å¼ã€æ ¡å†…è€ƒã€å‡ºæ„¿æ—¶é—´ã€å‡ºæ„¿ææ–™ã€æˆç»©è¦æ±‚ã€åˆæ ¼æƒ…å†µç­‰
"""
import json
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Optional, Any

# å¯é€‰ä¾èµ–
try:
    import requests
    from bs4 import BeautifulSoup
    import pdfplumber
    CRAWLER_AVAILABLE = True
except ImportError:
    CRAWLER_AVAILABLE = False
    print("âš ï¸  è­¦å‘Šï¼šç¼ºå°‘çˆ¬è™«ä¾èµ–åº“ã€‚è¯·å®‰è£…ï¼špip install requests beautifulsoup4 pdfplumber")

# æ–‡ä»¶è·¯å¾„
CSV_PATH = Path(__file__).parent.parent.parent / "å­¦æ ¡æ€»è§ˆ.csv"
URL_MAPPING_PATH = Path(__file__).parent.parent.parent / "crawled_data" / "university_urls.json"
OUTPUT_DIR = Path(__file__).parent.parent.parent / "crawled_data" / "unified_crawl_results"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

REQUEST_DELAY = 2  # ç§’


class UnifiedCrawler:
    """ç»Ÿä¸€çˆ¬å–æ¡†æ¶"""
    
    def __init__(self):
        self.results = []
        self.statistics = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "extraction_stats": defaultdict(int)
        }
    
    def extract_basic_info(self, text: str, soup: Any) -> Dict[str, Any]:
        """æå–åŸºç¡€ä¿¡æ¯"""
        result = {
            "å­¦éƒ¨": None,
            "å­¦ç§‘": None,
            "åœ°ç†ä½ç½®": None,
            "æ–‡ç†": None,
            "status": "not_found"
        }
        
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„ç¼–å†™æå–é€»è¾‘
        # ç¤ºä¾‹ï¼šæŸ¥æ‰¾åŒ…å«"å­¦éƒ¨"çš„æ–‡æœ¬
        # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
        
        return result
    
    def extract_period_info(self, text: str, soup: Any) -> Dict[str, Any]:
        """æå–æœŸæ•°ä¿¡æ¯"""
        result = {
            "åŸå§‹è¡¨è¿°": None,
            "æ‰€æœ‰å¯èƒ½çš„è¡¨è¿°": [],
            "status": "not_found"
        }
        
        # æœŸæ•°å…³é”®è¯
        period_keywords = [
            "å‰æœŸ", "å¾ŒæœŸ", "å¾ŒæœŸ", "ç¬¬1æœŸ", "ç¬¬2æœŸ", "ç¬¬3æœŸ", "ç¬¬4æœŸ",
            "ç¬¬ä¸€æœŸ", "ç¬¬äºŒæœŸ", "ç¬¬ä¸‰æœŸ", "ç¬¬å››æœŸ",
            "â… æœŸ", "â…¡æœŸ", "â…¢æœŸ", "â…£æœŸ",
            "Aæ–¹å¼", "Bæ–¹å¼", "Cæ–¹å¼",
            "å‰æœŸé¸æŠœ", "å¾ŒæœŸé¸æŠœ", "å˜ç‹¬é¸æŠœ",
            "æ¸¡æ—¥å‰", "2æœˆå®Ÿæ–½", "3æœˆå®Ÿæ–½", "åªæœ‰ä¸€æœŸ"
        ]
        
        found_periods = []
        for keyword in period_keywords:
            if keyword in text:
                # æå–åŒ…å«å…³é”®è¯çš„å¥å­
                pattern = f'.{{0,50}}{re.escape(keyword)}.{{0,50}}'
                matches = re.findall(pattern, text)
                found_periods.extend(matches[:3])
        
        if found_periods:
            result["åŸå§‹è¡¨è¿°"] = found_periods[0]
            result["æ‰€æœ‰å¯èƒ½çš„è¡¨è¿°"] = list(set(found_periods))
            result["status"] = "found"
        
        return result
    
    def extract_selection_method_info(self, text: str, soup: Any) -> Dict[str, Any]:
        """æå–é€‰è€ƒæ–¹å¼ä¿¡æ¯"""
        result = {
            "åŸå§‹è¡¨è¿°": None,
            "æ‰€æœ‰å¯èƒ½çš„è¡¨è¿°": [],
            "status": "not_found"
        }
        
        method_keywords = [
            "å¤–å›½äººå…¥è©¦", "å¤–å›½äººç‰¹åˆ¥é¸æŠœ", "å¤–å›½äººé¸æŠœ",
            "ä¸€èˆ¬å…¥è©¦", "ä¸€èˆ¬é¸æŠœ",
            "æ¨è–¦å…¥è©¦", "æ¨è–¦é¸æŠœ", "å­¦æ ¡æ¨è–¦",
            "AOå…¥è©¦", "AOé¸æŠœ",
            "ç·åˆå‹é¸æŠœ", "ç·åˆè©•ä¾¡å‹",
            "EJUåˆ©ç”¨", "EJUåˆ©ç”¨å‹",
            "æ ¡å†…è€ƒ", "æ›¸é¡é¸è€ƒ", "é¢æ¥"
        ]
        
        found_methods = []
        for keyword in method_keywords:
            if keyword in text:
                pattern = f'.{{0,50}}{re.escape(keyword)}.{{0,50}}'
                matches = re.findall(pattern, text)
                found_methods.extend(matches[:3])
        
        if found_methods:
            result["åŸå§‹è¡¨è¿°"] = found_methods[0]
            result["æ‰€æœ‰å¯èƒ½çš„è¡¨è¿°"] = list(set(found_methods))
            result["status"] = "found"
        
        return result
    
    def extract_exam_info(self, text: str, soup: Any) -> Dict[str, Any]:
        """æå–æ ¡å†…è€ƒä¿¡æ¯"""
        result = {
            "æœ‰æ— ": None,
            "ä¸€æ¬¡é€‰è€ƒ": {
                "åç§°": None,
                "æ—¶é—´": None,
                "å½¢å¼": None,
                "status": "not_found"
            },
            "äºŒæ¬¡é€‰è€ƒ": {
                "åç§°": None,
                "æ—¶é—´": None,
                "å½¢å¼": None,
                "status": "not_found"
            },
            "status": "not_found"
        }
        
        # æŸ¥æ‰¾æ ¡å†…è€ƒç›¸å…³ä¿¡æ¯
        exam_keywords = ["æ ¡å†…è€ƒ", "æ ¡å†…è©¦é¨“", "é¢æ¥", "å°è«–æ–‡", "ç­†è¨˜è©¦é¨“"]
        has_exam = any(keyword in text for keyword in exam_keywords)
        
        if has_exam:
            result["æœ‰æ— "] = "æœ‰"
            result["status"] = "found"
            
            # æŸ¥æ‰¾ä¸€æ¬¡/äºŒæ¬¡é€‰è€ƒ
            # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„é€»è¾‘æ¥æå–å…·ä½“ä¿¡æ¯
        
        return result
    
    def extract_application_time_info(self, text: str, soup: Any) -> Dict[str, Any]:
        """æå–å‡ºæ„¿æ—¶é—´ä¿¡æ¯"""
        result = {
            "ç½‘ä¸Šå‡ºæ„¿å¼€å§‹": None,
            "ç½‘ä¸Šå‡ºæ„¿æˆªæ­¢": None,
            "é‚®å¯„å¼€å§‹": None,
            "é‚®å¯„æˆªæ­¢": None,
            "å¿…ç€/æ¶ˆå°": None,
            "status": "not_found"
        }
        
        # æŸ¥æ‰¾æ—¥æœŸä¿¡æ¯
        date_pattern = r'(\d{4})[å¹´/](\d{1,2})[æœˆ/](\d{1,2})[æ—¥]?'
        dates = re.findall(date_pattern, text)
        
        # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„é€»è¾‘æ¥è¯†åˆ«å“ªä¸ªæ—¥æœŸå¯¹åº”å“ªä¸ªå­—æ®µ
        
        return result
    
    def extract_application_materials_info(self, text: str, soup: Any) -> Dict[str, Any]:
        """æå–å‡ºæ„¿ææ–™ä¿¡æ¯"""
        result = {
            "ææ–™æ¸…å•": [],
            "æ¨èä¿¡è¦æ±‚": None,
            "å‡ºæ„¿æµç¨‹": None,
            "status": "not_found"
        }
        
        # æŸ¥æ‰¾ææ–™ç›¸å…³å…³é”®è¯
        material_keywords = ["å…¥å­¦å¿—æ„¿ä¹¦", "æˆç»©è¯æ˜ä¹¦", "æ¯•ä¸šè¯æ˜ä¹¦", "æ¨èä¿¡", "ç ”ç©¶è®¡åˆ’ä¹¦"]
        found_materials = []
        
        for keyword in material_keywords:
            if keyword in text:
                found_materials.append(keyword)
        
        if found_materials:
            result["ææ–™æ¸…å•"] = found_materials
            result["status"] = "found"
        
        # æŸ¥æ‰¾æ¨èä¿¡è¦æ±‚
        if "æ¨èä¿¡" in text or "æ¨è–¦" in text:
            # æå–æ¨èä¿¡ç›¸å…³å¥å­
            pattern = r'.{0,100}(æ¨èä¿¡|æ¨è–¦).{0,100}'
            matches = re.findall(pattern, text)
            if matches:
                result["æ¨èä¿¡è¦æ±‚"] = matches[0]
        
        return result
    
    def extract_score_requirements_info(self, text: str, soup: Any) -> Dict[str, Any]:
        """æå–æˆç»©è¦æ±‚ä¿¡æ¯"""
        result = {
            "EJUç§‘ç›®": {
                "éœ€è¦çš„ç§‘ç›®": [],
                "æ¨èåˆ†æ•°": {},
                "status": "not_found"
            },
            "è‹±è¯­": {
                "æ˜¯å¦éœ€è¦": None,
                "æˆç»©ç±»å‹": None,
                "æ¨èåˆ†æ•°": None,
                "status": "not_found"
            },
            "JLPT": {
                "æ˜¯å¦éœ€è¦": None,
                "ç­‰çº§è¦æ±‚": None,
                "åˆ†æ•°è¦æ±‚": None,
                "status": "not_found"
            }
        }
        
        # æŸ¥æ‰¾EJUç§‘ç›®
        eju_keywords = ["æ—¥è¯­", "æ—¥æœ¬èª", "æ•°å­¦", "æ•°å­¦ã‚³ãƒ¼ã‚¹1", "æ•°å­¦ã‚³ãƒ¼ã‚¹2", "ç·åˆç§‘ç›®", "ç‰©ç†", "åŒ–å­¦", "ç”Ÿç‰©"]
        found_eju = []
        for keyword in eju_keywords:
            if keyword in text:
                found_eju.append(keyword)
        
        if found_eju:
            result["EJUç§‘ç›®"]["éœ€è¦çš„ç§‘ç›®"] = list(set(found_eju))
            result["EJUç§‘ç›®"]["status"] = "found"
        
        # æŸ¥æ‰¾è‹±è¯­è¦æ±‚
        if "TOEFL" in text or "æ‰˜ç¦" in text:
            result["è‹±è¯­"]["æ˜¯å¦éœ€è¦"] = "è¦"
            result["è‹±è¯­"]["æˆç»©ç±»å‹"] = "TOEFL"
            result["è‹±è¯­"]["status"] = "found"
        elif "TOEIC" in text:
            result["è‹±è¯­"]["æ˜¯å¦éœ€è¦"] = "è¦"
            result["è‹±è¯­"]["æˆç»©ç±»å‹"] = "TOEIC"
            result["è‹±è¯­"]["status"] = "found"
        elif "IELTS" in text:
            result["è‹±è¯­"]["æ˜¯å¦éœ€è¦"] = "è¦"
            result["è‹±è¯­"]["æˆç»©ç±»å‹"] = "IELTS"
            result["è‹±è¯­"]["status"] = "found"
        
        # æŸ¥æ‰¾JLPTè¦æ±‚
        if "JLPT" in text or "N1" in text or "N2" in text:
            result["JLPT"]["æ˜¯å¦éœ€è¦"] = "è¦"
            # æå–ç­‰çº§
            if "N1" in text:
                result["JLPT"]["ç­‰çº§è¦æ±‚"] = "N1"
            elif "N2" in text:
                result["JLPT"]["ç­‰çº§è¦æ±‚"] = "N2"
            result["JLPT"]["status"] = "found"
        
        return result
    
    def extract_admission_stats_info(self, text: str, soup: Any) -> Dict[str, Any]:
        """æå–åˆæ ¼æƒ…å†µä¿¡æ¯"""
        result = {
            "æŠ¥å½•æ¯”": {},
            "åˆæ ¼äººæˆç»©": None,
            "status": "not_found"
        }
        
        # æŸ¥æ‰¾æŠ¥å½•æ¯”ç›¸å…³ä¿¡æ¯
        # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„é€»è¾‘
        
        return result
    
    def crawl_university(self, university_name: str, department_name: str, url: str) -> Dict[str, Any]:
        """çˆ¬å–å•ä¸ªå¤§å­¦/å­¦éƒ¨çš„æ‰€æœ‰ä¿¡æ¯"""
        result = {
            "university": university_name,
            "department": department_name,
            "crawled_at": datetime.now().isoformat(),
            "source_url": url,
            "åŸºç¡€ä¿¡æ¯": {},
            "æœŸæ•°ä¿¡æ¯": {},
            "é€‰è€ƒæ–¹å¼": {},
            "æ ¡å†…è€ƒä¿¡æ¯": {},
            "å‡ºæ„¿æ—¶é—´": {},
            "å‡ºæ„¿ææ–™": {},
            "æˆç»©è¦æ±‚": {},
            "åˆæ ¼æƒ…å†µ": {},
            "æå–è´¨é‡": {
                "å®Œæ•´åº¦": 0.0,
                "éœ€è¦äººå·¥å®¡æ ¸": False,
                "æå–é—®é¢˜": []
            },
            "status": "pending"
        }
        
        if not CRAWLER_AVAILABLE:
            result["status"] = "error"
            result["æå–è´¨é‡"]["æå–é—®é¢˜"].append("ç¼ºå°‘çˆ¬è™«ä¾èµ–åº“")
            return result
        
        try:
            # è®¿é—®é¡µé¢
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                result["status"] = "error"
                result["æå–è´¨é‡"]["æå–é—®é¢˜"].append(f"HTTPé”™è¯¯: {response.status_code}")
                return result
            
            # è§£æHTML
            soup = BeautifulSoup(response.content, 'html.parser')
            text = extract_text_from_page(soup)
            
            # æå–æ‰€æœ‰ä¿¡æ¯
            result["åŸºç¡€ä¿¡æ¯"] = self.extract_basic_info(text, soup)
            result["æœŸæ•°ä¿¡æ¯"] = self.extract_period_info(text, soup)
            result["é€‰è€ƒæ–¹å¼"] = self.extract_selection_method_info(text, soup)
            result["æ ¡å†…è€ƒä¿¡æ¯"] = self.extract_exam_info(text, soup)
            result["å‡ºæ„¿æ—¶é—´"] = self.extract_application_time_info(text, soup)
            result["å‡ºæ„¿ææ–™"] = self.extract_application_materials_info(text, soup)
            result["æˆç»©è¦æ±‚"] = self.extract_score_requirements_info(text, soup)
            result["åˆæ ¼æƒ…å†µ"] = self.extract_admission_stats_info(text, soup)
            
            # è®¡ç®—å®Œæ•´åº¦
            total_fields = 8  # 8ä¸ªä¸»è¦ä¿¡æ¯ç±»åˆ«
            found_fields = sum(1 for key in ["åŸºç¡€ä¿¡æ¯", "æœŸæ•°ä¿¡æ¯", "é€‰è€ƒæ–¹å¼", "æ ¡å†…è€ƒä¿¡æ¯", 
                                            "å‡ºæ„¿æ—¶é—´", "å‡ºæ„¿ææ–™", "æˆç»©è¦æ±‚", "åˆæ ¼æƒ…å†µ"]
                             if result[key].get("status") == "found")
            result["æå–è´¨é‡"]["å®Œæ•´åº¦"] = found_fields / total_fields
            
            result["status"] = "success"
            self.statistics["successful"] += 1
            
        except Exception as e:
            result["status"] = "error"
            result["æå–è´¨é‡"]["æå–é—®é¢˜"].append(str(e))
            self.statistics["failed"] += 1
        
        return result
    
    def crawl_from_excel(self):
        """ä»Excelè¯»å–æ•°æ®å¹¶çˆ¬å–"""
        print("=" * 60)
        print("ç»Ÿä¸€çˆ¬å–æ¡†æ¶")
        print("=" * 60)
        print()
        
        if not CSV_PATH.exists():
            print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {CSV_PATH}")
            return
        
        # è¯»å–URLæ˜ å°„è¡¨
        if not URL_MAPPING_PATH.exists():
            print(f"âš ï¸  æ‰¾ä¸åˆ°URLæ˜ å°„è¡¨: {URL_MAPPING_PATH}")
            print("   è¯·å…ˆåˆ›å»ºURLæ˜ å°„è¡¨ï¼ˆå‚è€ƒ university_urls_template.jsonï¼‰")
            return
        
        with open(URL_MAPPING_PATH, "r", encoding="utf-8") as f:
            url_mapping = json.load(f)
        
        # è¯»å–Excel
        df = pd.read_csv(CSV_PATH, encoding='utf-8-sig')
        
        print(f"ğŸ“– è¯»å–æ•°æ®:")
        print(f"   Excelè®°å½•æ•°: {len(df)} æ¡")
        print(f"   URLæ˜ å°„æ•°: {len(url_mapping)} æ‰€å¤§å­¦")
        print()
        
        # çˆ¬å–æ¯ä¸ªå¤§å­¦/å­¦éƒ¨
        for _, row in df.iterrows():
            uni = row["å¤§å­¦"]
            dept = row["å­¦éƒ¨"]
            
            # è·å–URL
            url = None
            if uni in url_mapping:
                if isinstance(url_mapping[uni], str):
                    url = url_mapping[uni]
                elif isinstance(url_mapping[uni], dict):
                    url = url_mapping[uni].get("main_admission_url") or url_mapping[uni].get("main")
            
            if not url:
                print(f"âš ï¸  è·³è¿‡ {uni} - {dept}: æ²¡æœ‰URL")
                continue
            
            print(f"ğŸ•·ï¸  çˆ¬å–: {uni} - {dept}")
            result = self.crawl_university(uni, dept, url)
            self.results.append(result)
            self.statistics["total_processed"] += 1
            
            # å»¶è¿Ÿ
            time.sleep(REQUEST_DELAY)
        
        # ä¿å­˜ç»“æœ
        self.save_results()
    
    def save_results(self):
        """ä¿å­˜çˆ¬å–ç»“æœ"""
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        results_path = OUTPUT_DIR / f"crawl_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_statistics_report()
        
        print()
        print("=" * 60)
        print("çˆ¬å–å®Œæˆï¼")
        print("=" * 60)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ç”Ÿæˆ")
    
    def generate_statistics_report(self):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        # ç»Ÿè®¡å„ç§è¡¨è¿°
        period_statistics = defaultdict(int)
        method_statistics = defaultdict(int)
        
        for result in self.results:
            if result["æœŸæ•°ä¿¡æ¯"].get("åŸå§‹è¡¨è¿°"):
                period_statistics[result["æœŸæ•°ä¿¡æ¯"]["åŸå§‹è¡¨è¿°"]] += 1
            if result["é€‰è€ƒæ–¹å¼"].get("åŸå§‹è¡¨è¿°"):
                method_statistics[result["é€‰è€ƒæ–¹å¼"]["åŸå§‹è¡¨è¿°"]] += 1
        
        stats = {
            "çˆ¬å–ç»Ÿè®¡": {
                "æ€»å¤„ç†æ•°": self.statistics["total_processed"],
                "æˆåŠŸ": self.statistics["successful"],
                "å¤±è´¥": self.statistics["failed"]
            },
            "æœŸæ•°è¡¨è¿°ç»Ÿè®¡": dict(sorted(period_statistics.items(), key=lambda x: x[1], reverse=True)),
            "é€‰è€ƒæ–¹å¼è¡¨è¿°ç»Ÿè®¡": dict(sorted(method_statistics.items(), key=lambda x: x[1], reverse=True))
        }
        
        stats_path = OUTPUT_DIR / "statistics_report.json"
        with open(stats_path, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {stats_path}")


def extract_text_from_page(soup):
    """ä»é¡µé¢æå–æ–‡æœ¬"""
    for script in soup(["script", "style"]):
        script.decompose()
    text = soup.get_text()
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    return '\n'.join(chunk for chunk in chunks if chunk)


if __name__ == "__main__":
    if not CRAWLER_AVAILABLE:
        print("=" * 60)
        print("âš ï¸  ç¼ºå°‘çˆ¬è™«ä¾èµ–åº“")
        print("=" * 60)
        print()
        print("è¯·å®‰è£…ä¾èµ–ï¼š")
        print("  pip install requests beautifulsoup4 pdfplumber")
        print()
        print("æˆ–è€…å…ˆè¿è¡Œæ•°æ®æå–è„šæœ¬ï¼ˆä¸éœ€è¦çˆ¬è™«åº“ï¼‰ï¼š")
        print("  python3 scripts/crawlers/simple_crawl_classification.py")
    else:
        crawler = UnifiedCrawler()
        crawler.crawl_from_excel()
