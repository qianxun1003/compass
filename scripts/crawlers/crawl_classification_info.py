#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬å–å„å¤§å­¦å®˜ç½‘çš„æœŸæ•°å’Œé€‰è€ƒæ–¹å¼åˆ†ç±»ä¿¡æ¯
ç›®æ ‡ï¼šæ”¶é›†æ‰€æœ‰å¤§å­¦å®˜ç½‘çš„å®é™…è¡¨è¿°ï¼Œä¸ºå»ºç«‹æ ‡å‡†åˆ†ç±»ä½“ç³»æä¾›æ•°æ®åŸºç¡€
"""
import pandas as pd
import requests
from bs4 import BeautifulSoup
from pathlib import Path
import json
import time
import re
from datetime import datetime
from collections import defaultdict
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser

# æ–‡ä»¶è·¯å¾„
CSV_PATH = Path(__file__).parent.parent.parent / "å­¦æ ¡æ€»è§ˆ.csv"
OUTPUT_DIR = Path(__file__).parent.parent.parent / "crawled_data" / "classification_info"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# è¯·æ±‚é…ç½®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# å»¶è¿Ÿé…ç½®ï¼ˆé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›ï¼‰
REQUEST_DELAY = 2  # ç§’

def check_robots_txt(url):
    """æ£€æŸ¥robots.txt"""
    try:
        parsed = urlparse(url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
        rp = RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        return rp.can_fetch(HEADERS['User-Agent'], url)
    except:
        return True  # å¦‚æœæ— æ³•è¯»å–robots.txtï¼Œé»˜è®¤å…è®¸

def extract_text_from_page(soup):
    """ä»é¡µé¢æå–æ–‡æœ¬å†…å®¹"""
    # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
    for script in soup(["script", "style"]):
        script.decompose()
    
    # è·å–æ–‡æœ¬
    text = soup.get_text()
    # æ¸…ç†æ–‡æœ¬
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = '\n'.join(chunk for chunk in chunks if chunk)
    return text

def find_admission_page_url(university_name, base_url=None):
    """
    å°è¯•æ‰¾åˆ°æ‹›ç”Ÿç›¸å…³é¡µé¢URL
    å¸¸è§çš„æ‹›ç”Ÿé¡µé¢è·¯å¾„ï¼š
    - /admission/
    - /nyushi/
    - /entrance/
    - /admissions/
    - /international/
    """
    if not base_url:
        # å¦‚æœæ²¡æœ‰æä¾›base_urlï¼Œå°è¯•å¸¸è§çš„å¤§å­¦åŸŸåæ ¼å¼
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        return None
    
    common_paths = [
        "/admission/",
        "/nyushi/",
        "/entrance/",
        "/admissions/",
        "/international/",
        "/admission/undergraduate/",
        "/nyushi/gaikokujin/",
        "/entrance/international/",
    ]
    
    for path in common_paths:
        url = urljoin(base_url, path)
        try:
            response = requests.head(url, headers=HEADERS, timeout=5)
            if response.status_code == 200:
                return url
        except:
            continue
    
    return None

def extract_period_info(text, soup):
    """æå–æœŸæ•°ç›¸å…³ä¿¡æ¯"""
    period_keywords = [
        "å‰æœŸ", "å¾ŒæœŸ", "å¾ŒæœŸ", "ç¬¬1æœŸ", "ç¬¬2æœŸ", "ç¬¬3æœŸ", "ç¬¬4æœŸ",
        "ç¬¬ä¸€æœŸ", "ç¬¬äºŒæœŸ", "ç¬¬ä¸‰æœŸ", "ç¬¬å››æœŸ",
        "â… æœŸ", "â…¡æœŸ", "â…¢æœŸ", "â…£æœŸ",
        "Aæ–¹å¼", "Bæ–¹å¼", "Cæ–¹å¼",
        "å‰æœŸé¸æŠœ", "å¾ŒæœŸé¸æŠœ", "å˜ç‹¬é¸æŠœ",
        "æ¸¡æ—¥å‰", "2æœˆå®Ÿæ–½", "3æœˆå®Ÿæ–½"
    ]
    
    found_periods = []
    text_lower = text.lower()
    
    # æŸ¥æ‰¾åŒ…å«æœŸæ•°å…³é”®è¯çš„å¥å­
    for keyword in period_keywords:
        if keyword in text:
            # å°è¯•æå–åŒ…å«å…³é”®è¯çš„å¥å­
            pattern = f'.{{0,50}}{re.escape(keyword)}.{{0,50}}'
            matches = re.findall(pattern, text)
            found_periods.extend(matches[:3])  # æœ€å¤šä¿å­˜3ä¸ªåŒ¹é…
    
    return list(set(found_periods))  # å»é‡

def extract_selection_method_info(text, soup):
    """æå–é€‰è€ƒæ–¹å¼ç›¸å…³ä¿¡æ¯"""
    method_keywords = [
        "å¤–å›½äººå…¥è©¦", "å¤–å›½äººç‰¹åˆ¥é¸æŠœ", "å¤–å›½äººé¸æŠœ",
        "ä¸€èˆ¬å…¥è©¦", "ä¸€èˆ¬é¸æŠœ",
        "æ¨è–¦å…¥è©¦", "æ¨è–¦é¸æŠœ", "å­¦æ ¡æ¨è–¦",
        "AOå…¥è©¦", "AOé¸æŠœ",
        "ç·åˆå‹é¸æŠœ", "ç·åˆè©•ä¾¡å‹",
        "EJUåˆ©ç”¨", "EJUåˆ©ç”¨å‹",
        "æ ¡å†…è€ƒ", "æ›¸é¡é¸è€ƒ", "é¢æ¥"
    ]
    
    found_methods = []
    text_lower = text.lower()
    
    for keyword in method_keywords:
        if keyword in text:
            pattern = f'.{{0,50}}{re.escape(keyword)}.{{0,50}}'
            matches = re.findall(pattern, text)
            found_methods.extend(matches[:3])
    
    return list(set(found_methods))

def extract_exam_info(text, soup):
    """æå–æ ¡å†…è€ƒç›¸å…³ä¿¡æ¯"""
    exam_keywords = [
        "æ ¡å†…è€ƒ", "æ ¡å†…è©¦é¨“", "é¢æ¥", "å°è«–æ–‡", "ç­†è¨˜è©¦é¨“",
        "ä¸€æ¬¡é¸è€ƒ", "äºŒæ¬¡é¸è€ƒ", "ç¬¬ä¸€æ¬¡", "ç¬¬äºŒæ¬¡",
        "æ›¸é¡é¸è€ƒã®ã¿", "ç´”æ›¸é¡"
    ]
    
    found_exams = []
    
    for keyword in exam_keywords:
        if keyword in text:
            pattern = f'.{{0,50}}{re.escape(keyword)}.{{0,50}}'
            matches = re.findall(pattern, text)
            found_exams.extend(matches[:3])
    
    return list(set(found_exams))

def extract_eju_subjects_info(text, soup):
    """æå–EJUç§‘ç›®ç›¸å…³ä¿¡æ¯"""
    eju_keywords = [
        "EJU", "æ—¥æœ¬ç•™å­¦è©¦é¨“", "æ—¥æœ¬èª", "æ•°å­¦", "æ•°å­¦ã‚³ãƒ¼ã‚¹1", "æ•°å­¦ã‚³ãƒ¼ã‚¹2",
        "ç·åˆç§‘ç›®", "ç‰©ç†", "åŒ–å­¦", "ç”Ÿç‰©", "ç†ç§‘"
    ]
    
    found_eju = []
    
    for keyword in eju_keywords:
        if keyword in text:
            pattern = f'.{{0,50}}{re.escape(keyword)}.{{0,50}}'
            matches = re.findall(pattern, text)
            found_eju.extend(matches[:3])
    
    return list(set(found_eju))

def crawl_university_info(university_name, department_name=None):
    """
    çˆ¬å–å•ä¸ªå¤§å­¦çš„åˆ†ç±»ä¿¡æ¯
    æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªåŸºç¡€æ¡†æ¶ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å„å¤§å­¦ç½‘ç«™ç»“æ„è°ƒæ•´
    """
    result = {
        "university": university_name,
        "department": department_name or "",
        "period_info": [],
        "selection_method_info": [],
        "exam_info": [],
        "eju_subjects_info": [],
        "source_urls": [],
        "crawled_at": datetime.now().isoformat(),
        "status": "pending"
    }
    
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µå®ç°
    # ç”±äºæ¯ä¸ªå¤§å­¦çš„ç½‘ç«™ç»“æ„ä¸åŒï¼Œå¯èƒ½éœ€è¦ï¼š
    # 1. ç»´æŠ¤ä¸€ä¸ªå¤§å­¦URLæ˜ å°„è¡¨
    # 2. ä¸ºæ¯ä¸ªå¤§å­¦ç¼–å†™ç‰¹å®šçš„çˆ¬è™«é€»è¾‘
    # 3. æˆ–è€…ä½¿ç”¨é€šç”¨çš„é¡µé¢è§£æé€»è¾‘
    
    # ç¤ºä¾‹ï¼šå°è¯•è®¿é—®å¸¸è§çš„æ‹›ç”Ÿé¡µé¢
    # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
    
    return result

def crawl_from_excel():
    """ä»Excelè¯»å–å¤§å­¦åˆ—è¡¨å¹¶çˆ¬å–"""
    print("=" * 60)
    print("å¼€å§‹çˆ¬å–å„å¤§å­¦å®˜ç½‘çš„åˆ†ç±»ä¿¡æ¯")
    print("=" * 60)
    print()
    
    if not CSV_PATH.exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {CSV_PATH}")
        return
    
    # è¯»å–CSV
    print(f"ğŸ“– è¯»å–æ–‡ä»¶: {CSV_PATH}")
    df = pd.read_csv(CSV_PATH, encoding='utf-8-sig')
    print(f"   æ€»è®°å½•æ•°: {len(df)} æ¡")
    print()
    
    # è·å–å”¯ä¸€çš„å¤§å­¦åˆ—è¡¨
    universities = df["å¤§å­¦"].unique()
    print(f"ğŸ“Š å‘ç° {len(universities)} æ‰€ä¸åŒçš„å¤§å­¦")
    print()
    
    # ç»Ÿè®¡æ¯ä¸ªå¤§å­¦çš„å­¦éƒ¨æ•°
    dept_count = df.groupby("å¤§å­¦")["å­¦éƒ¨"].nunique()
    print("å„å¤§å­¦çš„å­¦éƒ¨æ•°ï¼ˆå‰10æ‰€ï¼‰ï¼š")
    for uni, count in dept_count.head(10).items():
        print(f"  {uni}: {count} ä¸ªå­¦éƒ¨")
    print()
    
    # æ”¶é›†æ‰€æœ‰ä¸åŒçš„è¡¨è¿°
    all_periods = defaultdict(list)
    all_methods = defaultdict(list)
    all_exams = defaultdict(list)
    all_eju = defaultdict(list)
    
    # ä»ç°æœ‰æ•°æ®ä¸­æå–åŸå§‹è¡¨è¿°ï¼ˆä½œä¸ºå‚è€ƒï¼‰
    print("=" * 60)
    print("ä»ç°æœ‰æ•°æ®ä¸­æå–åŸå§‹è¡¨è¿°ï¼ˆä½œä¸ºå‚è€ƒï¼‰")
    print("=" * 60)
    print()
    
    for _, row in df.iterrows():
        uni = row["å¤§å­¦"]
        dept = row["å­¦éƒ¨"]
        period = str(row.get("ç¬¬å‡ æœŸ", "")).strip()
        method = str(row.get("æ–¹å¼", "")).strip()
        exam = str(row.get("æ ¡å†…è€ƒå½¢å¼", "")).strip()
        eju = str(row.get("éœ€è¦EJUç§‘ç›®", "")).strip()
        
        if period:
            all_periods[period].append({"university": uni, "department": dept})
        if method:
            all_methods[method].append({"university": uni, "department": dept})
        if exam:
            all_exams[exam].append({"university": uni, "department": dept})
        if eju:
            all_eju[eju].append({"university": uni, "department": dept})
    
    # ä¿å­˜ç°æœ‰æ•°æ®çš„ç»Ÿè®¡
    print("æœŸæ•°è¡¨è¿°ç»Ÿè®¡ï¼ˆç°æœ‰æ•°æ®ï¼‰ï¼š")
    for period, records in sorted(all_periods.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
        print(f"  {period:30s} : {len(records):5d} æ¬¡")
    print()
    
    print("é€‰è€ƒæ–¹å¼è¡¨è¿°ç»Ÿè®¡ï¼ˆç°æœ‰æ•°æ®ï¼‰ï¼š")
    for method, records in sorted(all_methods.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
        print(f"  {method:30s} : {len(records):5d} æ¬¡")
    print()
    
    # ä¿å­˜åˆ°JSON
    existing_data = {
        "periods": {k: len(v) for k, v in all_periods.items()},
        "methods": {k: len(v) for k, v in all_methods.items()},
        "exams": {k: len(v) for k, v in all_exams.items()},
        "eju_subjects": {k: len(v) for k, v in all_eju.items()},
    }
    
    existing_data_path = OUTPUT_DIR / "existing_data_statistics.json"
    with open(existing_data_path, "w", encoding="utf-8") as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç°æœ‰æ•°æ®ç»Ÿè®¡å·²ä¿å­˜åˆ°: {existing_data_path}")
    print()
    
    print("=" * 60)
    print("ä¸‹ä¸€æ­¥ï¼šçˆ¬å–å„å¤§å­¦å®˜ç½‘")
    print("=" * 60)
    print()
    print("âš ï¸  æ³¨æ„ï¼š")
    print("1. ç”±äºæ¯ä¸ªå¤§å­¦çš„ç½‘ç«™ç»“æ„ä¸åŒï¼Œéœ€è¦ä¸ºæ¯ä¸ªå¤§å­¦ç¼–å†™ç‰¹å®šçš„çˆ¬è™«é€»è¾‘")
    print("2. æˆ–è€…ç»´æŠ¤ä¸€ä¸ªå¤§å­¦URLæ˜ å°„è¡¨ï¼Œæ‰‹åŠ¨æŒ‡å®šæ¯ä¸ªå¤§å­¦çš„æ‹›ç”Ÿé¡µé¢URL")
    print("3. å»ºè®®å…ˆåšè¯•ç‚¹ï¼Œé€‰æ‹©10-20æ‰€ä»£è¡¨æ€§å¤§å­¦æµ‹è¯•")
    print()
    print("å»ºè®®çš„å®æ–½æ–¹æ¡ˆï¼š")
    print("1. å…ˆåˆ›å»ºä¸€ä¸ªå¤§å­¦URLæ˜ å°„è¡¨ï¼ˆæ‰‹åŠ¨å¡«å†™å„å¤§å­¦çš„æ‹›ç”Ÿé¡µé¢URLï¼‰")
    print("2. ç¼–å†™é€šç”¨çš„é¡µé¢è§£æé€»è¾‘")
    print("3. ä¸ºç‰¹æ®Šç½‘ç«™ç¼–å†™ç‰¹å®šçš„çˆ¬è™«é€»è¾‘")
    print("4. é€æ­¥æ‰©å±•åˆ°æ‰€æœ‰å¤§å­¦")


if __name__ == "__main__":
    crawl_from_excel()
