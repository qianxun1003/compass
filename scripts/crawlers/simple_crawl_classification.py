#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆçˆ¬è™«ï¼šä»ç°æœ‰æ•°æ®ä¸­æå–ä¿¡æ¯ï¼Œå¹¶å°è¯•è®¿é—®å®˜ç½‘è¡¥å……
ç›®æ ‡ï¼šæ”¶é›†æ‰€æœ‰ä¸åŒçš„æœŸæ•°å’Œé€‰è€ƒæ–¹å¼è¡¨è¿°
"""
import pandas as pd
from pathlib import Path
import json
from datetime import datetime
from collections import defaultdict

# å¯é€‰ä¾èµ–ï¼ˆç”¨äºåç»­çˆ¬å–ï¼‰
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

# æ–‡ä»¶è·¯å¾„
CSV_PATH = Path(__file__).parent.parent.parent / "å­¦æ ¡æ€»è§ˆ.csv"
OUTPUT_DIR = Path(__file__).parent.parent.parent / "crawled_data" / "classification_info"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

def extract_all_classifications_from_excel():
    """
    ä»ç°æœ‰Excelæ•°æ®ä¸­æå–æ‰€æœ‰ä¸åŒçš„è¡¨è¿°
    è¿™æ˜¯ç¬¬ä¸€æ­¥ï¼šå…ˆäº†è§£ç°æœ‰æ•°æ®ä¸­æœ‰å“ªäº›è¡¨è¿°
    """
    print("=" * 60)
    print("ç¬¬ä¸€æ­¥ï¼šä»ç°æœ‰æ•°æ®ä¸­æå–æ‰€æœ‰è¡¨è¿°")
    print("=" * 60)
    print()
    
    if not CSV_PATH.exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {CSV_PATH}")
        return
    
    df = pd.read_csv(CSV_PATH, encoding='utf-8-sig')
    print(f"ğŸ“– è¯»å–æ–‡ä»¶: {CSV_PATH}")
    print(f"   æ€»è®°å½•æ•°: {len(df)} æ¡")
    print()
    
    # æ”¶é›†æ‰€æœ‰ä¸åŒçš„è¡¨è¿°
    periods = defaultdict(list)
    methods = defaultdict(list)
    exams = defaultdict(list)
    eju_subjects = defaultdict(list)
    
    for _, row in df.iterrows():
        uni = str(row.get("å¤§å­¦", "")).strip()
        dept = str(row.get("å­¦éƒ¨", "")).strip()
        
        # æœŸæ•°
        period = str(row.get("ç¬¬å‡ æœŸ", "")).strip()
        if period and period != "nan":
            periods[period].append({
                "university": uni,
                "department": dept,
                "source": "existing_data"
            })
        
        # é€‰è€ƒæ–¹å¼
        method = str(row.get("æ–¹å¼", "")).strip()
        if method and method != "nan":
            methods[method].append({
                "university": uni,
                "department": dept,
                "source": "existing_data"
            })
        
        # æ ¡å†…è€ƒ
        exam = str(row.get("æ ¡å†…è€ƒå½¢å¼", "")).strip()
        if exam and exam != "nan":
            exams[exam].append({
                "university": uni,
                "department": dept,
                "source": "existing_data"
            })
        
        # EJUç§‘ç›®
        eju = str(row.get("éœ€è¦EJUç§‘ç›®", "")).strip()
        if eju and eju != "nan":
            eju_subjects[eju].append({
                "university": uni,
                "department": dept,
                "source": "existing_data"
            })
    
    # ä¿å­˜ç»“æœ
    result = {
        "extracted_at": datetime.now().isoformat(),
        "source": "å­¦æ ¡æ€»è§ˆ.csv",
        "total_records": len(df),
        "periods": {
            "unique_count": len(periods),
            "distribution": {k: len(v) for k, v in sorted(periods.items(), key=lambda x: len(x[1]), reverse=True)},
            "details": {k: v[:5] for k, v in periods.items()}  # æ¯ç§è¡¨è¿°çš„å‰5ä¸ªç¤ºä¾‹
        },
        "methods": {
            "unique_count": len(methods),
            "distribution": {k: len(v) for k, v in sorted(methods.items(), key=lambda x: len(x[1]), reverse=True)},
            "details": {k: v[:5] for k, v in methods.items()}
        },
        "exams": {
            "unique_count": len(exams),
            "distribution": {k: len(v) for k, v in sorted(exams.items(), key=lambda x: len(x[1]), reverse=True)},
            "details": {k: v[:5] for k, v in exams.items()}
        },
        "eju_subjects": {
            "unique_count": len(eju_subjects),
            "distribution": {k: len(v) for k, v in sorted(eju_subjects.items(), key=lambda x: len(x[1]), reverse=True)},
            "details": {k: v[:5] for k, v in eju_subjects.items()}
        }
    }
    
    # ä¿å­˜åˆ°JSON
    output_path = OUTPUT_DIR / "existing_classifications.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print("âœ… æå–å®Œæˆï¼")
    print()
    print(f"ğŸ“Š ç»Ÿè®¡ç»“æœï¼š")
    print(f"   æœŸæ•°è¡¨è¿°ï¼š{len(periods)} ç§")
    print(f"   é€‰è€ƒæ–¹å¼è¡¨è¿°ï¼š{len(methods)} ç§")
    print(f"   æ ¡å†…è€ƒè¡¨è¿°ï¼š{len(exams)} ç§")
    print(f"   EJUç§‘ç›®è¡¨è¿°ï¼š{len(eju_subjects)} ç§")
    print()
    print(f"âœ… è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    print()
    
    # æ˜¾ç¤ºå‰10ç§æœ€å¸¸è§çš„è¡¨è¿°
    print("=" * 60)
    print("æœŸæ•°è¡¨è¿°ï¼ˆå‰10ç§ï¼‰ï¼š")
    print("=" * 60)
    for period, records in sorted(periods.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
        print(f"  {period:30s} : {len(records):5d} æ¬¡")
        print(f"    ç¤ºä¾‹: {records[0]['university']} - {records[0]['department']}")
    print()
    
    print("=" * 60)
    print("é€‰è€ƒæ–¹å¼è¡¨è¿°ï¼ˆå‰10ç§ï¼‰ï¼š")
    print("=" * 60)
    for method, records in sorted(methods.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
        print(f"  {method:30s} : {len(records):5d} æ¬¡")
        print(f"    ç¤ºä¾‹: {records[0]['university']} - {records[0]['department']}")
    print()
    
    return result

def generate_crawl_plan():
    """
    ç”Ÿæˆçˆ¬å–è®¡åˆ’
    åŸºäºç°æœ‰æ•°æ®ï¼Œè¯†åˆ«éœ€è¦é‡ç‚¹å…³æ³¨çš„å¤§å­¦
    """
    print("=" * 60)
    print("ç”Ÿæˆçˆ¬å–è®¡åˆ’")
    print("=" * 60)
    print()
    
    df = pd.read_csv(CSV_PATH, encoding='utf-8-sig')
    
    # ç»Ÿè®¡æ¯ä¸ªå¤§å­¦çš„è®°å½•æ•°
    uni_counts = df.groupby("å¤§å­¦").size().sort_values(ascending=False)
    
    print("å»ºè®®çš„çˆ¬å–ä¼˜å…ˆçº§ï¼ˆæŒ‰è®°å½•æ•°æ’åºï¼Œå‰20æ‰€ï¼‰ï¼š")
    print()
    for i, (uni, count) in enumerate(uni_counts.head(20).items(), 1):
        print(f"{i:2d}. {uni:30s} : {count:4d} æ¡è®°å½•")
    
    print()
    print("=" * 60)
    print("ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®")
    print("=" * 60)
    print()
    print("1. âœ… å·²å®Œæˆï¼šä»ç°æœ‰æ•°æ®æå–æ‰€æœ‰è¡¨è¿°")
    print("2. â³ ä¸‹ä¸€æ­¥ï¼šåˆ›å»ºå¤§å­¦URLæ˜ å°„è¡¨")
    print("   - æ–‡ä»¶ï¼šcrawled_data/university_urls.json")
    print("   - æ¨¡æ¿ï¼šcrawled_data/university_urls_template.json")
    print("   - å»ºè®®ï¼šå…ˆå¡«å†™å‰20æ‰€å¤§å­¦çš„URL")
    print()
    print("3. â³ ç„¶åï¼šç¼–å†™çˆ¬è™«è„šæœ¬è®¿é—®å®˜ç½‘")
    print("   - æå–å®˜ç½‘ä¸Šçš„å®é™…è¡¨è¿°")
    print("   - ä¸ç°æœ‰æ•°æ®å¯¹æ¯”")
    print("   - å‘ç°æ–°çš„è¡¨è¿°")
    print()
    print("4. â³ æœ€åï¼šåŸºäºçˆ¬å–ç»“æœå»ºç«‹æ ‡å‡†åˆ†ç±»")
    print("   - ç»Ÿè®¡æ‰€æœ‰ä¸åŒçš„è¡¨è¿°")
    print("   - è¯†åˆ«åŒä¹‰è¯å’Œå˜ä½“")
    print("   - å»ºç«‹æ ‡å‡†åˆ†ç±»ä½“ç³»")


if __name__ == "__main__":
    # ç¬¬ä¸€æ­¥ï¼šä»ç°æœ‰æ•°æ®æå–
    result = extract_all_classifications_from_excel()
    
    # ç”Ÿæˆçˆ¬å–è®¡åˆ’
    generate_crawl_plan()
    
    print()
    print("=" * 60)
    print("å®Œæˆï¼")
    print("=" * 60)
    print()
    print("ğŸ“ ä¸‹ä¸€æ­¥ï¼š")
    print("1. æŸ¥çœ‹ crawled_data/classification_info/existing_classifications.json")
    print("2. å¡«å†™å¤§å­¦URLæ˜ å°„è¡¨ï¼ˆcrawled_data/university_urls_template.jsonï¼‰")
    print("3. è¿è¡Œçˆ¬è™«è„šæœ¬è®¿é—®å®˜ç½‘ï¼ˆå¾…å®ç°ï¼‰")
