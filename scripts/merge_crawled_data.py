#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®‰å…¨åˆå¹¶çˆ¬å–æ•°æ®åˆ°ä¸»Excelæ–‡ä»¶
åªæ·»åŠ æ–°æ•°æ®ï¼Œä¸è¦†ç›–ç°æœ‰æ•°æ®
"""
import pandas as pd
from pathlib import Path
from datetime import datetime
import shutil

# æ–‡ä»¶è·¯å¾„
MAIN_EXCEL = Path(__file__).parent.parent / "å­¦éƒ¨å­¦æ ¡ä¸€è§ˆè¡¨.xlsx"
CRAWLED_EXCEL = Path(__file__).parent.parent / "crawled_data" / "crawled_schools_review.xlsx"
BACKUP_DIR = Path(__file__).parent.parent / "backups"

def backup_excel():
    """å¤‡ä»½ä¸»Excelæ–‡ä»¶"""
    if not BACKUP_DIR.exists():
        BACKUP_DIR.mkdir(parents=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = BACKUP_DIR / f"å­¦éƒ¨å­¦æ ¡ä¸€è§ˆè¡¨_{timestamp}.xlsx"
    shutil.copy2(MAIN_EXCEL, backup_path)
    print(f"âœ… å·²å¤‡ä»½ä¸»Excelåˆ°: {backup_path}")
    return backup_path

def is_duplicate(row1, row2):
    """åˆ¤æ–­ä¸¤æ¡è®°å½•æ˜¯å¦é‡å¤ï¼ˆåŸºäºå¤§å­¦+å­¦éƒ¨ï¼‰"""
    return (
        row1.get("å¤§å­¦", "").strip() == row2.get("å¤§å­¦", "").strip() and
        row1.get("å­¦éƒ¨", "").strip() == row2.get("å­¦éƒ¨", "").strip()
    )

def merge_data():
    """åˆå¹¶æ•°æ®"""
    if not MAIN_EXCEL.exists():
        print(f"âŒ æ‰¾ä¸åˆ°ä¸»Excelæ–‡ä»¶: {MAIN_EXCEL}")
        return False
    
    if not CRAWLED_EXCEL.exists():
        print(f"âŒ æ‰¾ä¸åˆ°çˆ¬å–æ•°æ®æ–‡ä»¶: {CRAWLED_EXCEL}")
        print(f"   è¯·å…ˆè¿è¡Œçˆ¬è™«å¹¶å®¡æ ¸æ•°æ®")
        return False
    
    # å¤‡ä»½
    backup_path = backup_excel()
    
    # è¯»å–ä¸»Excel
    print("ğŸ“– è¯»å–ä¸»Excelæ–‡ä»¶...")
    main_df = pd.read_excel(MAIN_EXCEL, sheet_name="å­¦æ ¡æ€»è§ˆ")
    print(f"   ç°æœ‰æ•°æ®: {len(main_df)} æ¡")
    
    # è¯»å–çˆ¬å–æ•°æ®
    print("ğŸ“– è¯»å–çˆ¬å–æ•°æ®...")
    crawled_df = pd.read_excel(CRAWLED_EXCEL, sheet_name="å­¦æ ¡æ€»è§ˆ")
    print(f"   çˆ¬å–æ•°æ®: {len(crawled_df)} æ¡")
    
    # æ‰¾å‡ºæ–°æ•°æ®ï¼ˆä¸é‡å¤çš„ï¼‰
    new_rows = []
    duplicate_count = 0
    
    for _, crawled_row in crawled_df.iterrows():
        is_dup = False
        for _, main_row in main_df.iterrows():
            if is_duplicate(crawled_row.to_dict(), main_row.to_dict()):
                is_dup = True
                duplicate_count += 1
                break
        
        if not is_dup:
            new_rows.append(crawled_row)
    
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"   - æ–°æ•°æ®: {len(new_rows)} æ¡")
    print(f"   - é‡å¤æ•°æ®: {duplicate_count} æ¡ï¼ˆå·²è·³è¿‡ï¼‰")
    
    if len(new_rows) == 0:
        print("\nâœ… æ²¡æœ‰æ–°æ•°æ®éœ€è¦åˆå¹¶")
        return True
    
    # ç¡®è®¤åˆå¹¶
    print(f"\nâš ï¸  å‡†å¤‡æ·»åŠ  {len(new_rows)} æ¡æ–°æ•°æ®åˆ°ä¸»Excel")
    confirm = input("ç¡®è®¤åˆå¹¶ï¼Ÿ(y/n): ").strip().lower()
    
    if confirm != 'y':
        print("âŒ å·²å–æ¶ˆåˆå¹¶")
        return False
    
    # åˆå¹¶æ•°æ®
    new_df = pd.DataFrame(new_rows)
    merged_df = pd.concat([main_df, new_df], ignore_index=True)
    
    # ä¿å­˜
    with pd.ExcelWriter(MAIN_EXCEL, engine='openpyxl') as writer:
        merged_df.to_excel(writer, sheet_name="å­¦æ ¡æ€»è§ˆ", index=False)
    
    print(f"\nâœ… åˆå¹¶å®Œæˆï¼")
    print(f"   - åŸæ•°æ®: {len(main_df)} æ¡")
    print(f"   - æ–°å¢: {len(new_rows)} æ¡")
    print(f"   - æ€»è®¡: {len(merged_df)} æ¡")
    print(f"   - å¤‡ä»½æ–‡ä»¶: {backup_path}")
    print(f"\nğŸ“ ä¸‹ä¸€æ­¥: è¿è¡Œ python3 export_school_data.py æ›´æ–°JSONæ–‡ä»¶")
    
    return True

if __name__ == "__main__":
    print("=" * 60)
    print("æ•°æ®åˆå¹¶å·¥å…·")
    print("=" * 60)
    print()
    
    merge_data()
