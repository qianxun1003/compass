#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°†ç»Ÿä¸€çˆ¬å–æ¡†æ¶çš„è¾“å‡ºæ•°æ®åˆå¹¶åˆ°ç°æœ‰Excelç»“æ„
æ™ºèƒ½åˆå¹¶ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨çˆ¬å–æ•°æ®ï¼Œä¿ç•™ç°æœ‰æ•°æ®ï¼ˆå¦‚æœçˆ¬å–æ•°æ®ç¼ºå¤±ï¼‰
"""
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import shutil

# æ–‡ä»¶è·¯å¾„
EXCEL_PATH = Path(__file__).parent.parent / "å­¦éƒ¨å­¦æ ¡ä¸€è§ˆè¡¨.xlsx"
CRAWLED_DATA_DIR = Path(__file__).parent.parent / "crawled_data" / "unified_crawl_results"
BACKUP_DIR = Path(__file__).parent.parent / "backups"

def backup_excel():
    """å¤‡ä»½Excelæ–‡ä»¶"""
    if not BACKUP_DIR.exists():
        BACKUP_DIR.mkdir(parents=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = BACKUP_DIR / f"å­¦éƒ¨å­¦æ ¡ä¸€è§ˆè¡¨_{timestamp}.xlsx"
    shutil.copy2(EXCEL_PATH, backup_path)
    print(f"âœ… å·²å¤‡ä»½Excelåˆ°: {backup_path}")
    return backup_path

def load_crawled_data():
    """åŠ è½½çˆ¬å–çš„æ•°æ®"""
    # æŸ¥æ‰¾æœ€æ–°çš„çˆ¬å–ç»“æœæ–‡ä»¶
    json_files = list(CRAWLED_DATA_DIR.glob("crawl_results_*.json"))
    if not json_files:
        print(f"âŒ æ‰¾ä¸åˆ°çˆ¬å–ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œçˆ¬è™«")
        return None
    
    # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(json_files, key=lambda p: p.stat().st_mtime)
    print(f"ğŸ“– è¯»å–çˆ¬å–ç»“æœ: {latest_file}")
    
    with open(latest_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    print(f"   æ‰¾åˆ° {len(data)} æ¡çˆ¬å–æ•°æ®")
    return data

def map_crawled_to_excel_format(crawled_item):
    """
    å°†çˆ¬å–æ•°æ®æ˜ å°„åˆ°Excelæ ¼å¼
    """
    excel_row = {}
    
    # åŸºç¡€ä¿¡æ¯
    basic = crawled_item.get("åŸºç¡€ä¿¡æ¯", {})
    excel_row["å¤§å­¦"] = crawled_item.get("university", "")
    excel_row["å­¦éƒ¨"] = basic.get("å­¦éƒ¨") or crawled_item.get("department", "")
    excel_row["å­¦ç§‘"] = basic.get("å­¦ç§‘", "")
    excel_row["ä½ç½®"] = basic.get("åœ°ç†ä½ç½®", "")
    excel_row["æ–‡ç†"] = basic.get("æ–‡ç†", "")
    
    # æœŸæ•°å’Œé€‰è€ƒæ–¹å¼ï¼ˆéœ€è¦æ ‡å‡†åŒ–ï¼Œè¿™é‡Œå…ˆä¿å­˜åŸå§‹è¡¨è¿°ï¼‰
    period_info = crawled_item.get("æœŸæ•°ä¿¡æ¯", {})
    excel_row["ç¬¬å‡ æœŸ"] = period_info.get("åŸå§‹è¡¨è¿°", "")
    
    method_info = crawled_item.get("é€‰è€ƒæ–¹å¼", {})
    excel_row["æ–¹å¼"] = method_info.get("åŸå§‹è¡¨è¿°", "")
    
    # æ ¡å†…è€ƒä¿¡æ¯
    exam_info = crawled_item.get("æ ¡å†…è€ƒä¿¡æ¯", {})
    exam_format_parts = []
    if exam_info.get("ä¸€æ¬¡é€‰è€ƒ", {}).get("å½¢å¼"):
        exam_format_parts.append(exam_info["ä¸€æ¬¡é€‰è€ƒ"]["å½¢å¼"])
    if exam_info.get("äºŒæ¬¡é€‰è€ƒ", {}).get("å½¢å¼"):
        exam_format_parts.append(exam_info["äºŒæ¬¡é€‰è€ƒ"]["å½¢å¼"])
    excel_row["æ ¡å†…è€ƒå½¢å¼"] = ", ".join(exam_format_parts) if exam_format_parts else ""
    excel_row["æ ¡å†…è€ƒæ—¶é—´1"] = exam_info.get("ä¸€æ¬¡é€‰è€ƒ", {}).get("æ—¶é—´", "")
    excel_row["æ ¡å†…è€ƒæ—¶é—´2"] = exam_info.get("äºŒæ¬¡é€‰è€ƒ", {}).get("æ—¶é—´", "")
    
    # å‡ºæ„¿æ—¶é—´
    time_info = crawled_item.get("å‡ºæ„¿æ—¶é—´", {})
    excel_row["ç½‘ä¸Šå‡ºæ„¿å¼€å§‹æ—¶é—´"] = time_info.get("ç½‘ä¸Šå‡ºæ„¿å¼€å§‹", "")
    excel_row["ç½‘ä¸Šå‡ºæ„¿æˆªæ­¢æ—¶é—´"] = time_info.get("ç½‘ä¸Šå‡ºæ„¿æˆªæ­¢", "")
    excel_row["é‚®å¯„å¼€å§‹æ—¶é—´"] = time_info.get("é‚®å¯„å¼€å§‹", "")
    excel_row["é‚®å¯„æˆªæ­¢æ—¶é—´"] = time_info.get("é‚®å¯„æˆªæ­¢", "")
    excel_row["å¿…ç€/æ¶ˆå°"] = time_info.get("å¿…ç€/æ¶ˆå°", "")
    
    # EJUå’Œæˆç»©è¦æ±‚
    score_info = crawled_item.get("æˆç»©è¦æ±‚", {})
    eju_info = score_info.get("EJUç§‘ç›®", {})
    excel_row["éœ€è¦EJUç§‘ç›®"] = ", ".join(eju_info.get("éœ€è¦çš„ç§‘ç›®", []))
    excel_row["èƒ½ä½¿ç”¨EJU"] = ""  # éœ€è¦ä»å…¶ä»–ä¿¡æ¯æå–
    
    english_info = score_info.get("è‹±è¯­", {})
    excel_row["è‹±è¯­"] = english_info.get("æ˜¯å¦éœ€è¦", "")
    
    jlpt_info = score_info.get("JLPT", {})
    excel_row["JLPT"] = jlpt_info.get("æ˜¯å¦éœ€è¦", "")
    
    # æ–°å­—æ®µï¼ˆéœ€è¦æ·»åŠ åˆ°Excelï¼‰
    excel_row["è‹±è¯­æˆç»©ç±»å‹"] = english_info.get("æˆç»©ç±»å‹", "")
    excel_row["è‹±è¯­æˆç»©æ¨èåˆ†æ•°"] = english_info.get("æ¨èåˆ†æ•°", "")
    excel_row["JLPTç­‰çº§"] = jlpt_info.get("ç­‰çº§è¦æ±‚", "")
    excel_row["JLPTåˆ†æ•°è¦æ±‚"] = jlpt_info.get("åˆ†æ•°è¦æ±‚", "")
    excel_row["EJUæ¨èåˆ†æ•°"] = json.dumps(eju_info.get("æ¨èåˆ†æ•°", {}), ensure_ascii=False) if eju_info.get("æ¨èåˆ†æ•°") else ""
    
    # å‡ºæ„¿ææ–™ï¼ˆæ–°å­—æ®µï¼‰
    materials_info = crawled_item.get("å‡ºæ„¿ææ–™", {})
    excel_row["å‡ºæ„¿ææ–™"] = ", ".join(materials_info.get("ææ–™æ¸…å•", []))
    excel_row["æ¨èä¿¡è¦æ±‚"] = materials_info.get("æ¨èä¿¡è¦æ±‚", "")
    excel_row["å‡ºæ„¿æµç¨‹"] = materials_info.get("å‡ºæ„¿æµç¨‹", "")
    
    # åˆæ ¼æƒ…å†µï¼ˆæ–°å­—æ®µï¼‰
    admission_info = crawled_item.get("åˆæ ¼æƒ…å†µ", {})
    ratio_info = admission_info.get("æŠ¥å½•æ¯”", {})
    if ratio_info:
        excel_row["æŠ¥å½•æ¯”ï¼ˆ2024ï¼‰"] = ratio_info.get("2024", {}).get("æ¯”ä¾‹", "")
        excel_row["æŠ¥å½•æ¯”ï¼ˆ2023ï¼‰"] = ratio_info.get("2023", {}).get("æ¯”ä¾‹", "")
    
    return excel_row

def merge_data():
    """åˆå¹¶æ•°æ®"""
    print("=" * 60)
    print("æ•°æ®åˆå¹¶å·¥å…·")
    print("=" * 60)
    print()
    
    if not EXCEL_PATH.exists():
        print(f"âŒ æ‰¾ä¸åˆ°Excelæ–‡ä»¶: {EXCEL_PATH}")
        return False
    
    # åŠ è½½çˆ¬å–æ•°æ®
    crawled_data = load_crawled_data()
    if not crawled_data:
        return False
    
    # å¤‡ä»½Excel
    backup_path = backup_excel()
    
    # è¯»å–ç°æœ‰Excel
    print("ğŸ“– è¯»å–ç°æœ‰Excel...")
    df_existing = pd.read_excel(EXCEL_PATH, sheet_name="å­¦æ ¡æ€»è§ˆ")
    print(f"   ç°æœ‰æ•°æ®: {len(df_existing)} æ¡")
    print()
    
    # å°†çˆ¬å–æ•°æ®è½¬æ¢ä¸ºDataFrame
    print("ğŸ”„ è½¬æ¢çˆ¬å–æ•°æ®...")
    crawled_rows = []
    for item in crawled_data:
        row = map_crawled_to_excel_format(item)
        if row.get("å¤§å­¦"):
            crawled_rows.append(row)
    
    if not crawled_rows:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„çˆ¬å–æ•°æ®")
        return False
    
    df_crawled = pd.DataFrame(crawled_rows)
    print(f"   çˆ¬å–æ•°æ®: {len(df_crawled)} æ¡")
    print()
    
    # æ™ºèƒ½åˆå¹¶ç­–ç•¥
    print("ğŸ”€ åˆå¹¶æ•°æ®...")
    print("   ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨çˆ¬å–æ•°æ®ï¼Œä¿ç•™ç°æœ‰æ•°æ®ï¼ˆå¦‚æœçˆ¬å–æ•°æ®ç¼ºå¤±ï¼‰")
    print()
    
    # åˆ›å»ºåˆå¹¶åçš„DataFrame
    # ä»¥ç°æœ‰Excelçš„åˆ—ä¸ºåŸºç¡€
    merged_df = df_existing.copy()
    
    # ä¸ºæ¯ä¸ªå¤§å­¦/å­¦éƒ¨æ›´æ–°æ•°æ®
    update_count = 0
    add_count = 0
    
    for _, crawled_row in df_crawled.iterrows():
        uni = crawled_row.get("å¤§å­¦", "")
        dept = crawled_row.get("å­¦éƒ¨", "")
        
        if not uni:
            continue
        
        # æŸ¥æ‰¾ç°æœ‰æ•°æ®ä¸­æ˜¯å¦æœ‰åŒ¹é…çš„è®°å½•
        mask = (merged_df["å¤§å­¦"] == uni) & (merged_df["å­¦éƒ¨"] == dept)
        matching_rows = merged_df[mask]
        
        if len(matching_rows) > 0:
            # æ›´æ–°ç°æœ‰è®°å½•
            for idx in matching_rows.index:
                # åªæ›´æ–°çˆ¬å–æ•°æ®ä¸­æœ‰çš„å­—æ®µï¼ˆéç©ºï¼‰
                for col in crawled_row.index:
                    if pd.notna(crawled_row[col]) and str(crawled_row[col]).strip():
                        merged_df.at[idx, col] = crawled_row[col]
                update_count += 1
        else:
            # æ·»åŠ æ–°è®°å½•
            merged_df = pd.concat([merged_df, pd.DataFrame([crawled_row])], ignore_index=True)
            add_count += 1
    
    print(f"ğŸ“Š åˆå¹¶ç»Ÿè®¡:")
    print(f"   - æ›´æ–°è®°å½•: {update_count} æ¡")
    print(f"   - æ–°å¢è®°å½•: {add_count} æ¡")
    print(f"   - æ€»è®°å½•æ•°: {len(merged_df)} æ¡")
    print()
    
    # ç¡®è®¤ä¿å­˜
    print(f"âš ï¸  å‡†å¤‡ä¿å­˜åˆå¹¶åçš„æ•°æ®")
    confirm = input("ç¡®è®¤ä¿å­˜ï¼Ÿ(y/n): ").strip().lower()
    
    if confirm != 'y':
        print("âŒ å·²å–æ¶ˆä¿å­˜")
        return False
    
    # ä¿å­˜
    with pd.ExcelWriter(EXCEL_PATH, engine='openpyxl') as writer:
        merged_df.to_excel(writer, sheet_name="å­¦æ ¡æ€»è§ˆ", index=False)
    
    print()
    print("âœ… åˆå¹¶å®Œæˆï¼")
    print(f"   - å¤‡ä»½æ–‡ä»¶: {backup_path}")
    print(f"   - æ›´æ–°è®°å½•: {update_count} æ¡")
    print(f"   - æ–°å¢è®°å½•: {add_count} æ¡")
    print()
    print("ğŸ“ ä¸‹ä¸€æ­¥:")
    print("   1. æ£€æŸ¥åˆå¹¶åçš„Excelæ•°æ®")
    print("   2. è¿è¡Œ: python3 export_school_data.py")
    print("   3. æ›´æ–°JSONæ–‡ä»¶")
    
    return True

if __name__ == "__main__":
    merge_data()
